% !TeX root = ../main.tex
\chapter{基于高质量图像分类数据扩展的预训练方法}
\label{cha:iclip}
语言-图像对比学习方法利用对比学习框架，通过在图文数据对中构建正负样本对的方式驱动模型训练。该方法将图像与其相应文本认定为正样本对，并将该图像与其他文本认定为负样本对。互联网数据中的文本来源多样，并包含丰富的语义信息。这些图文数据对帮助模型有效地学习图像中的物体、场景与文本中的类别、属性间的关联关系，从而使模型能够具备零样本开放集合图像识别能力。
然而，如图\ref{fig:iclip-overall}左侧所示，自动爬取的互联网图文数据对缺少额外的人工标注或校对流程，通常包含大量图文无关的噪声数据（如相机参数、推广信息等）以及关联关系模糊、判别性不强的图文数据对。这些低质量数据影响了CLIP方法的视觉表征对语义概念的建模效果，降低了视觉表征的判别能力。当前用于过滤低质量数据的启发式方法\cite{sharma-etal-2018-conceptual,changpinyo2021conceptual}，多基于图像分辨率与文本长度等简单规则，无法有效去除语义层面的噪声数据。因此，探寻高质量语义信号来源并降低图文数据对中噪声的影响，是提升CLIP方法预训练效果的关键路径，也是本章的研究目标。

\begin{figure}
  \centering
  \includegraphics[width=1.0\linewidth]{figures/论文-iCLIP-概要-v2.pdf}
  \caption{互联网图文数据对与图像分类数据的对比示意图}
  \label{fig:iclip-overall}
\end{figure}

% 拜读了zl大哥DNL的部分，没想到引言部分是直接翻译过来的
\section{引言}
\label{sec:iclip-intro}
本章提出将对比学习思想扩展到其他类型的图像-标注配对数据中，从而有效利用已有的人工标注数据作为对互联网图文数据对的补充，并实现不同标注形式预训练数据的有效融合。这种方式一方面为CLIP方法提供了额外的训练信号和图像样本，另一方面使得CLIP方法可以利用经过人工校对的语义标注信号作为视觉表征的精确锚点，以增强其判别效果，并促进视觉与语言表征更紧密的对齐效果。本章主要考虑将图像分类数据集作为对现有图文数据集的补充。
图像分类任务属于典型的视觉预训练任务。该任务的训练目标是在给定一组固定类别集合时，将每一张输入图像归类到对应的类别。作为有监督预训练方法中最常使用的任务，研究界已积累大量公开图像数据。如图\ref{fig:iclip-overall}右侧所示，广泛使用的ImageNet数据集\cite{deng2009imagenet}包含1400余万张图像的精细标注，并分为两万余个不同的类别。其中每个类别都通过词汇网络\cite{miller1995wordnet}（WordNet）获得清晰的语义界定。因此图像分类任务的语义信号标注质量更高，语义含义更明确，不同图像与标注之间混淆的情况较少，是互联网图文数据对的理想补充。
% 如果找到一种办法能有效利用这些现有的图像数据及其标注信息，并在实例级视觉-语言预训练方法中统一起来，将有效地提升整体数据的信噪比，并改善互联网图像文本对噪声导致的图文关联模糊问题。
% 同时，传统的图像分类任务往往仅限于一组预定义的固定类别集合，而CLIP方法天然具有开放类别的物体识别能力，这种表征融合的方法也很好地弥补了图像分类方法的不足。

% 本章将CLIP方法中的\{图像，文本\}实例级对比学习方法扩展到\{图像，文本，类别\}的三元组，在结合这两种视觉预训练方法的同时，充分利用对应的训练数据。
% 作为预实验，本章首先尝试了一个简单的多任务学习框架。该框架使用一个共享的视觉模型，并分别使用一个线性分类器进行图像分类任务训练和一个语言模型进行实例级多模态联合预训练。此时，\{图像，文本，类别\}的三元组关系通过分别建模\{图像，文本\}和\{图像，类别\}关系来实现。
% 虽然提升效果比较微弱，这种简单的多任务表征融合方法已经能够使两个单独的任务获益，但这一实验结果促进了本章沿着该方向的进一步探索。

为实现将图像分类数据引入图文数据对中以增强其整体数据质量，本章从CLIP方法的角度探讨了如何对传统的有监督图像分类方法进行重新阐释与理解。第一个方法是统一图像分类方法与CLIP方法的损失函数。通过对比两者的损失函数形式，可以发现两种训练方法都可视为一种特殊的分类任务，其中前者是固定类别集合的分类任务，后者是采样后的正负样本对间的分类任务。但两种方法的损失函数仍然有两个主要区别：1）损失函数形式不同。图像分类方法通常采用基于内积相似度的交叉熵损失函数\cite{alexnet}，而CLIP方法则使用基于余弦相似度的InfoNCE损失函数\cite{oord2018representation}。前者拟合能力更强，而后者具有更好的类别泛化能力。
% 1）损失函数形式不同。图像分类方法通常使用拟合能力较强的线性损失分类器。%由于线性损失的非归一化性质，该分类器驱动下的模型往往具有更好的拟合能力。
% 而语言-图像对比学习方法则采用对新类别泛化性更好的余弦损失分类器\cite{cao2020parametric,DengArcFace,wang2018cosface}。
2）分类器权重的参数化方法不同。图像分类方法采用直接参数化的类别分类器，而CLIP方法则使用语言模型重参数化分类器。前者适合固定类别分类任务，后者适合开放类别分类任务。
% 图像分类方法只处理固定类别集合分类任务，可以直接优化各个类别的中心权重。而语言-图像对比学习方法则使用语言模型来建模不同文本中包含的语义信息，可以理解为通过语言模型重参数化了各个类别的中心权重。这种共享语义表征建模方式允许对不同文本语义之间的内部关系进行建模以增强对不同概念间关系的理解。同时这种方式可以处理预训练过程中未曾出现的概念信息，实现开放集合图像识别任务。
因此本章提出一种修改图像分类损失函数的方法，从而使图像分类方法与CLIP方法在损失函数形式与分类器类型方面形成统一。

除了统一两种方法的损失函数之外，本章的第二个方法是借助外部专家知识库丰富图像类别标签的语义信息，以弥补图像分类数据和图文数据对在标注信息语义丰富度上的差异。由于图像分类数据中的类别标签注重简洁性和明确性，因此通常仅由一个或几个单词来表示。但不同的类别标签之间可能有概念上的重叠，这使得它们在指代特定概念时存在歧义。而图文数据对中提供语义信息的标注通常是一个包含丰富语义的自然语言描述。这种语义信息密度的差异影响了CLIP方法的语言模型学习效果。因此，本章提出外部专家知识库增强类别标签方法，利用字典、百科等外部知识库，为图像类别标签提供更丰富的语义信息，包括类别定义、相关描述、与其他概念的关联和区别等。这个过程类似于人类通过真实示例和字典解释来学习新词汇或概念，同时也建模了这些新概念与其他概念间的关系。得益于在图像分类方法中引入语言模型重参数化分类器的设计，这种包含复杂语义信息的外部知识可以通过语言模型来直接建模，从而有助于从CLIP方法的角度更深入地理解图像分类方法。

% 图像分类任务中类别名称为力求简洁明确，通常以一个或几个单词来表示。但不同的类别名称之间可能有概念上的重叠，这使得类别名称在指代特定语义时存在有歧义或多义的情况。
% 例如，ImageNet数据集中有一类别为``夜晚的鸟''（``night bird''），这一类别名称就与数据集中出现的其他类别有概念上的重叠，包括数据集中出现的``猫头鹰''（``owl''）、``夜莺''（``nightingale''）等类别。此外，``nightingale''还与知名历史人物弗洛伦丝·南丁格尔（``Florence Nightingale''）有词汇上的重合。% https://storage.googleapis.com/bit_models/imagenet21k_wordnet_lemmas.txt
% 与此同时，CLIP联合预训练方法使用图文数据对训练，其中的替代文本往往是包含丰富语义的完整句子。
% 为了进一步弥合图像分类任务和实例级视觉-语言多模态联合预训练任务之间的差距，
基于这些技术，本章提出了一种新的预训练方法iCLIP。iCLIP方法从损失函数和标注信息语义丰富度两个角度出发，通过对比学习的视角重新理解有监督图像分类方法，实现了在CLIP方法预训练过程中对高质量图像分类数据的有效融合。这不仅降低了图文数据对中噪声数据的影响，也扩展了CLIP方法可用的预训练数据来源。

实验结果首先表明，尽管内积相似度和直接参数化类别分类器是图像分类方法的标准做法，但是将CLIP方法的余弦损失函数和语言模型重参数化分类器引入图像分类方法，不会对图像分类任务本身性能造成影响。这一结果验证了iCLIP方法采用对比学习思想融合两种数据源进行预训练的可行性。此外，iCLIP方法提出的外部专家知识库增强方法显著缩小了图像分类数据和图文数据对在标注信息语义丰富度上的差距。实验表明将外部知识作为文本前缀或后缀与类别标签组合，能够显著提高模型在零样本开放集合图像识别任务上的性能，并提升了其在零样本图文跨模态检索任务上的表现。与简单的多任务预训练方法相比，iCLIP方法通过从损失函数和标注语义丰富度角度对齐图像分类方法与CLIP方法，从而有效融合两种预训练方式，显著提升了视觉与语言表征的对齐效果，并增强了视觉表征的语义判别能力。
此外，在迁移至下游视觉任务时，iCLIP方法也表现出更优的性能。相比于单图像分类方法或单CLIP方法基线，iCLIP方法在少样本图像识别\cite{imagnettransfer}、ADE20K数据集语义分割\cite{zhou2019ade}、LVIS数据集长尾类别目标检测\cite{gupta2019lvis}和Kinetics-400数据集视频动作识别\cite{kay2017kinetics}等任务上均取得了优于基线方法的性能。
% 这表明以实例级视觉-语言多模态联合预训练CLIP方法的形式重新理解和建模图像分类任务没有额外损失，因此可以

% 本章在不同规模的数据集组合实验中对该方法进行了验证。结果表明更高精度的图像分类数据明显增强了CLIP预训练对效果，使其在零样本物体识别和图文跨模态检索任务中的表现明显优于单独使用其中一种预训练方法或简单的多任务学习方法。

综上所述，本章的主要内容安排如下：
\begin{itemize}
    \item 第\ref{sec:iclip-intro}节讨论了互联网图文数据对中的语义信号噪声问题和引入高质量标注数据（例如图像分类数据）的潜在好处，并介绍了如何从损失函数和标注语义丰富度两个角度，以对比学习思想重新理解图像分类任务。
    \item 第\ref{sec:iclip-method}节介绍了本章的研究方法，包括从对比学习角度重新理解图像分类方法的具体设计以及引入外部专家知识库丰富图像类别语义信息的具体方法等，并提出融合后的预训练方法iCLIP的具体实现。
    \item 第\ref{sec:iclip-result}节介绍了本章的实验设置、评测指标和实验结果，以及针对各技术的消融实验结果。
    \item 第\ref{sec:iclip-summary}节对本章内容进行了总结。
\end{itemize}

\section{研究方法}
在第\ref{sec:iclip-compare}节中首先回顾了图像分类方法和CLIP方法的损失函数。之后在第\ref{sec:iclip-adapt-classification}节中提出从对比学习角度重新理解图像分类方法的具体设计，并在第\ref{sec:iclip-adapt-external}节中讨论了引入外部专家知识库丰富图像类别语义信息的做法。最后在第\ref{sec:iclip-unify}节中介绍了多数据源有效融合的预训练方法iCLIP的具体实现。

\label{sec:iclip-method}
\begin{figure}
  \centering
  \includegraphics[width=1.0\linewidth]{figures/iclip-compare.pdf}
  \caption*{图中的灰色网格表示各方法的训练标签，深色部分表示正确标签位置}
  \caption{图像分类方法、CLIP方法和融合后的iCLIP方法示意图}
  \label{fig:iclip-compare}
\end{figure}
\subsection{图像分类方法和CLIP方法对比}
\label{sec:iclip-compare}
\paragraph{图像分类方法的损失函数} 图像分类方法的训练数据$D^{c}$是一组\{图像，类别序号\}对，记为$D^{c}={(I_{i}, C_{i})}_{i=1}^{|D^{c}|}$，其中$I_{i}$和$C_{i}$分别表示第$i$个图像和对应的类别序号。因此，$C_{i}\in [1,N]$，其中$N$是数据中的所有类别数目。图像分类方法的训练目标是使用视觉模型$\theta_{I}$和直接参数化的类别分类器$h_{c}$预测给定图像的类别序号。分类器$h_{c}$是一个$N×H$大小的线性层，其参数记为$W$，其中$H$是视觉模型$\theta_{I}$输出的视觉表征维度。图\ref{fig:iclip-compare}（a）展示了批大小为$\mathcal{B}$时的情况。
视觉模型$\theta_{I}$将每个图像$I_{i}$转换为视觉表征：$v_{i}=\theta_{I}(I_{i})$，而分类器$h_{c}$计算$W$和$v_{i}$之间的相似度来预测该图像在所有类别上的概率分布$P_{i}\in R^{N}$。最后，图像分类方法在$P_{i}$和$C_{i}$之间应用交叉熵函数来计算训练损失，公式化为：
\begin{equation}
 \mathcal{L}=-\frac{1}{\left|\mathcal{D}^{c}\right|} \sum_{\left(I_{i}, C_{i}\right) \in \mathcal{D}^{c}} \log \frac{\exp \left(W_{C_{i}} \cdot v_{i}\right)}{\sum_{j=1}^{N} \exp \left(W_{j} \cdot v_{i}\right)},
 \label{eq:iclip-classification}
\end{equation}
其中$W_{j}$表示分类器参数中第$j$个类别对应的中心权重。

\paragraph{语言-图像对比学习方法的损失函数} CLIP方法的训练数据$D^{a}$是一组\{图像，文本\}对，记为$D^{a}={(I_{i}, T_{i}^{a})}_{i=1}^{|D^{a}|}$，其中$I_{i}$和$T^{a}_{i}$分别表示第$i$个图像和对应的文本。
CLIP方法的训练思想是通过视觉模型$\theta_{I}$和语言模型$\theta_{T}$缩小配对的视觉-语言表征之间的距离，同时扩大未配对的视觉-语言表征之间的距离。
图\ref{fig:iclip-compare}（b）展示了批大小为$\mathcal{B}$时的情况。
CLIP方法首先将图像$I_{i}$和文本$T_{i}^{a}$通过$\theta_{I}$和$\theta_{T}$转换为对应表征$v_{i}$和$s_{i}$，也即$v_{i}=\theta_{I}(I_{i})$且$s_{i}=\theta_{T}(T_{i}^{a})$。接着，CLIP方法应用InfoNCE对比损失函数\cite{oord2018representation}来计算训练损失。以当前图像$I_{i}$为中心的损失函数公式化为：
\begin{equation}
 % \mathcal{L}=-\frac{1}{\left|\mathcal{D}^{a}\right|} \sum_{\substack{\left(I_{i}, T_{i}^{a}\right) \\ \in \mathcal{D}^{a}}} log \frac{exp \left(cos \left(\theta_{T}\left(T_{i}^{a}\right), v_{i}\right) / \tau\right)}{\sum_{T_{j}^{a} \in \mathcal{T}^{a}} exp \left(cos \left(\theta_{T}\left(T_{j}^{a}\right), v_{i}\right) / \tau\right)}
 \mathcal{L}=-\frac{1}{\left|\mathcal{D}^{a}\right|} \sum_{\left(I_{i}, T_{i}^{a}\right) \in \mathcal{D}^{a}} \log \frac{\exp \left(\cos \left(s_{i}, v_{i}\right) / \tau\right)}{\sum_{T_{j}^{a} \in \mathcal{T}^{a}} \exp \left(\cos \left(s_{j}, v_{i}\right) / \tau\right)},
 \label{eq:iclip-clip}
\end{equation}
其中$\cos(\cdot, \cdot)$表示两个表征之间的余弦相似度，$\mathcal{T}^{a}$是当前批数据中的所有文本，表示待对比对象集合，包括一个和当前图像配对的正样本和$\mathcal{B}-1$个不配对的负样本，而$\tau$是一个缩放相似程度的超参数。由于CLIP方法对图像信息和文本信息的建模对称性，在训练过程中还有一个以当前文本$T_{i}^{a}$为中心的损失函数。该损失函数和公式\eqref{eq:iclip-clip}形式相似，只是将对比对象从当前批数据中的所有文本替换为所有图像。最终损失平均了两个损失函数。
 
\paragraph{损失函数对比} 通过对比图像分类任务和CLIP方法的损失函数，可以看出两种训练方法都可以被视作完成一种特殊的分类任务。前者在固定大小的图像类别集合中分类，后者则在给定批数据的所有文本或图像中分类。但是它们之间仍有三个主要区别：
\begin{itemize}
    \item 两种方法在损失函数形式上有区别。图像分类方法使用基于内积相似度的线性损失函数。由于内积相似度是一种非归一化的相似度，因此可以通过表征和参数模长建模更多信息。该损失函数对应的分类器往往具有更好的拟合能力。而CLIP方法采用基于余弦相似度的损失函数。这种损失函数对应的分类器更关注表征间的方向信息，更有利于类别泛化\cite{cao2020parametric,DengArcFace,wang2018cosface}。
    % 1）损失函数形式的区别。图像分类任务通常采用基于内积相似度的交叉熵损失，而语言-图像对比学习方法则使用基于余弦相似度的InfoNCE损失函数\cite{oord2018representation}。
    \item 两种方法在分类器权重的参数化方法上有区别。图像分类方法采用直接参数化的类别分类器。这种方法直接优化各个类别的中心权重，但只适合处理固定类别集合的分类任务。而CLIP方法则使用语言模型来建模不同文本中包含的语义信息，因此这种方式本质上是通过语言模型重参数化分类器权重，并在不同类别间共享了分类器参数。这种方式允许分类器构建不同类别间的复杂关系，并增强对不同语义概念的理解。同时借助语言模型的语义表达能力，这种分类器可以建模训练过程中未曾出现的语义概念，取得开放集合图像识别的效果。
    % 图像分类方法采用直接参数化的类别分类器，而语言-图像对比学习方法则使用语言模型重参数化分类器。
    \item 两种方法在标注信息的语义丰富度上有区别。图像分类方法使用的类别序号仅仅区分了不同类别，并不包含任何语义信息，而CLIP方法使用的图文数据对中的文本标注提供了关于图像内容的详细描述。
\end{itemize}


\subsection{从对比学习角度重新理解图像分类方法}
\label{sec:iclip-adapt-classification}
为了有效融合图像分类任务和CLIP方法，从而运用高质量图像分类数据进行增强，本节讨论了从对比学习角度重新理解图像分类方法的两个调整措施：对齐损失函数形式和统一分类器参数化方法。整体流程如图\ref{fig:iclip-adapt}所示。
% todo: 现在的写法里，a.2 & a.3要画成对齐损失函数和分类器参数化方法。

\begin{figure}
  \centering
  \includegraphics[width=1.0\linewidth]{figures/iclip-adapt-v2.pdf}
  \caption{从对比学习角度重新理解图像分类方法的调整措施}
  \label{fig:iclip-adapt}
\end{figure}

\paragraph{基于余弦相似度的图像分类损失函数} 如公式\eqref{eq:iclip-classification}所示，图像分类方法首先得到视觉表征$v_{i}$与直接参数化的类别分类器$h_{c}$之间的内积相似度，并归一化为概率分布$P_i$之后，与独热化类别序号之间应用交叉熵损失函数。这个损失函数形式与CLIP方法中使用的InfoNCE对比损失函数相似但不一致，使得两个方法无法有效融合。
为了消除这个差异，本节提出在图像分类方法中引入余弦相似度，并同样添加一个控制概率分布的温度系数$\tau$。此时，图像分类方法的损失函数形式与InfoNCE对比损失函数一致，只是采样的负样本为其他图像类别。基于余弦相似度的图像分类损失函数公式化为：
\begin{equation}
    \mathcal{L}=-\frac{1}{\left|\mathcal{D}^{c}\right|} \sum_{\left(I_{i}, C_{i}\right) \in \mathcal{D}^{c}} \log \frac{\exp \left(\cos \left(W_{C_{i}}, v_{i}\right) / \tau\right)}{\sum_{j=1}^{N} \exp \left(\cos \left(W_{j}, v_{i}\right) / \tau\right)}.
    \label{eq:iclip-cosine}
\end{equation}

余弦相似度是度量学习\cite{nguyen2010cosine}中的一种常见做法。通过在图像分类方法中引入余弦相似度，图像分类方法与CLIP方法在损失函数形式方面保持一致。后续实验表明，这种基于余弦相似度的图像分类方法可以取得与传统基于内积相似度的图像分类方法一致的分类性能。

\paragraph{在图像分类方法中引入语言模型重参数化分类器权重} 虽然公式\eqref{eq:iclip-cosine}对齐了两种方法的损失函数形式，但是两种方法使用的图像标注信息：图像分类数据中的类别序号和图文数据对中的文本标注，仍由直接参数化的类别分类器$h_{c}$和语言模型$\theta_{T}$分别建模。
在这种方式下，视觉模型接收到的训练信号由两个分类器单独给出，而且不同的语义概念信息无法在两个分类器间共享，因此无法充分结合文本标注语义概念丰富性和类别标签概念准确性的优势。
% 语言模型的学习信号并没有获得引入图像分类数据后的收益。
后续实验也表明，这种简单融合的多任务训练方法没有充分利用图像分类数据和图文数据对的优势，视觉表征与语言表征的对齐效果不佳。

为了解决这个问题，本节重新引入图像类别标签中蕴含的语义信息，并提出用语言模型重参数化分类器方法建模图像分类任务。
如前文所述，图像分类数据集ImageNet中的每一个类别序号都对应了WordNet中的一个同义词集合，因此每个类别序号都有其对应的标签文本。语言模型$\theta_{T}$重参数化分类器权重做法如下：先通过WordNet将类别序号$C_{i}$转化为其对应的标签文本$M_{i}$，并通过语言模型$\theta_{T}$提取$N$个不同类别对应的语言表征，从而动态生成分类器权重$W$。在图像分类方法中引入语言模型重参数化分类器权重后的公式为：
\begin{equation}
    \mathcal{L}=-\frac{1}{\left|\mathcal{D}^{c}\right|} \sum_{\left(I_{i}, M_{i}\right) \in \mathcal{D}^{c}} \log \frac{\exp \left(\cos \left(\theta_{T}\left(M_{i}\right), v_{i}\right) / \tau\right)}{\sum_{j=1}^{N} \exp \left(\cos \left(\theta_{T}\left(M_{j}\right), v_{i}\right) / \tau\right)}.
    \label{eq:iclip-wo-dictionary}
\end{equation}

此时，语言模型$\theta_{T}$不仅用于提取分类数据中判别性好的标签文本语义特征，还能整合图文数据对中丰富的语义概念。这一改进有助于从CLIP方法的角度更深入地理解图像分类方法。

\subsection{外部专家知识库增强图像类别语义方法} 

第\ref{sec:iclip-adapt-classification}节通过基于余弦相似度的图像分类损失函数和语言模型重参数化分类器权重方法，实现了从对比学习角度重新理解图像分类方法的目标，并在很大程度上统一了两种预训练方法。
但是两种方法使用的数据在标注信息的语义丰富度上仍有较大区别。图像分类方法中每个类别对应的标签文本$M_i$来自于WordNet中的同义词集合。为了保持图像类别的简洁明确，每个类别的标签文本通常仅由一个或几个单词来表示。与此同时，在CLIP方法使用的图文数据对中，每个图像对应的文本标注包含了丰富的语义信息，通常以完整语句的形式描述了图像中存在的物体对象、物体属性、对象间关系、上下文背景信息等等。
因此，为了进一步减少两种方法之间的差异，增强视觉表征的学习效果，需要在标注信息的语义丰富度上对两种方法进行对齐。

针对这个问题，本节提出引入外部专家知识库进行增强的方法，丰富了图像分类数据中各类别的语义信息。该方法流程如图\ref{fig:iclip-knowledge}所示。具体而言，对于分类数据中的每一个类别，该方法都从外部专家知识库中查询并构造相关的知识信息作为标签文本的补充。这个知识库可以是字典、百科、知识图谱或者大语言模型。
本节利用了WordNet作为外部专家知识库。如前文所述，每个图像类别在WordNet中都对应了一个同义词集合，而WordNet类似字典，为每个同义词集合编写了一条详细描述，用以明确定义该同义词集合所表达的语义概念，并提供一些相关知识以及与其他同义词之间的关联关系作为补充。
得到每个类别对应的详细描述之后，该方法通过一些语句模板，将其与原始类别标签文本组合，构成了语义丰富度增强后的类别标注。这种增强后的标注称为图像类别描述，记作$\mathcal{T}^{c}$。
以“银行”类别为例，利用WordNet进行语义扩充后的类别标注为：“这是一张银行的图片，关于银行的定义是接受存款并将资金用于贷款活动的金融机构”。
% https://web.stanford.edu/~jurafsky/slp3/G.pdf

\label{sec:iclip-adapt-external}
\begin{figure}
  \centering
  \includegraphics[width=1.0\linewidth]{figures/iclip-knowledge.pdf}
  \caption{外部专家知识库对类别标签的增强方法示意图}
  \label{fig:iclip-knowledge}
\end{figure}

% 此外，在增强后的句子中。知识库增强后的类别标签形式为：
% \begin{equation}
%     \mathcal{T}^{c}=\{\text{TEMPLATE}\}_{C_i},\{\text{NAME}\}_{C_{i}},\{\text{DESCRIPTION} \}_{C_{i}}
%     \label{eq:iclip-format-label}
% \end{equation}

 这种外部专家知识库增强后的类别描述具有与图文数据对的文本信息相似的语义丰富度，从而进一步对齐了图像分类方法与CLIP方法。
 此外，这些外部专家知识库为每个图像类别引入了更多细节信息，能够减少因为类别标签文本之间概念重叠造成的混淆现象。这种混淆现象是由于图像类别的标签文本信息比较省略，因此在类别数目较多的情况下存在指代特定语义时不同类别出现歧义的现象。
 例如，ImageNet数据集中有一类别为“夜晚的鸟”（“night bird”）。这一类别名称与数据集中出现的其他类别有概念上的重叠，包括“猫头鹰”（“owl”）和“夜莺”（“nightingale”）等。此外，“夜莺”的英文名称还与历史人物弗洛伦斯·南丁格尔护士（“Florence Nightingale”）有词汇上的相似。
 因此，仅仅依靠图像类别的标签文本难以区分不同类别间的细微差别，限制了视觉表征在预训练过程中学习到更精确的语义概念表示。
 但是WordNet提供了关于类别更明确的语义信息，将“夜晚的鸟”这一类别详细描述为“任何与夜相关的鸟类：猫头鹰、夜莺、夜鹰......”。因此引入外部专家知识库的方法不仅在标注信息语义丰富度上对齐了图像分类数据与图文数据对，还有助于模型区分不同类别的语义概念，进一步增强视觉表征的判别能力。

% 图像分类任务中类别名称为力求简洁明确，通常以一个或几个单词来表示。但不同的类别名称之间可能有概念上的重叠，这使得类别名称在指代特定语义时存在有歧义或多义的情况。
% 例如，ImageNet数据集中有一类别为``夜晚的鸟''（``night bird''），这一类别名称就与数据集中出现的其他类别有概念上的重叠，包括数据集中出现的``猫头鹰''（``owl''）、``夜莺''（``nightingale''）等类别。此外，``nightingale''还与知名历史人物弗洛伦丝·南丁格尔（``Florence Nightingale''）有词汇上的重合。% https://storage.googleapis.com/bit_models/imagenet21k_wordnet_lemmas.txt
% 与此同时，CLIP联合预训练方法使用图文数据对训练，其中的替代文本往往是包含丰富语义的完整句子。
% 为了进一步弥合图像分类任务和实例级视觉-语言多模态联合预训练任务之间的差距，

\subsection{有效融合的预训练方法iCLIP}
\label{sec:iclip-unify}
通过从对比学习角度重新理解图像分类方法并引入外部专家知识库增强图像类别，图像分类方法与CLIP方法在损失函数形式、分类器权重参数化方法和标注信息语义丰富度等三个方面对齐。基于此，本节提出了一个有效融合的预训练方法iCLIP。
如图\ref{fig:iclip-compare}（c）所示，iCLIP方法将高质量图像分类数据引入CLIP方法中，并用对比学习方法统一建模，其形式为：
\begin{equation}
   \mathcal{L}=-\frac{1}{|\mathcal{D}|} \sum_{\left(I_{i}, T_{i}\right) \in \mathcal{D}} \log \frac{\exp \left(\cos \left(\theta_{T}(T_{i}), \theta_{I}(I_{i})\right) / \tau\right)}{\sum_{T_{j} \in \mathcal{T}} \exp \left(\cos \left(\theta_{T}(T_{j}), \theta_{I}(I_{i})\right) / \tau\right)},
   \label{eq:iclip-unified}
\end{equation}
其中$D$是由图像分类数据$D^{c}$和图文数据对$D^{a}$组成的集合。而对比对象集合$\mathcal{T}$表示所有类别的图像类别描述集合$\mathcal{T}^{c}$，或者当前批数据中的图文对文本集合$\mathcal{T}^{a}$。因此，$T_{j}$可以是从$\mathcal{T}^{c}$中采样的图像类别描述，或者从$\mathcal{T}^{a}$中采样的文本标注。
值得注意的是，在融合后的iCLIP方法中，视觉模型$\theta_{I}$和语言模型$\theta_{T}$在图像分类数据和图文数据对之间共享。这种融合方式使得模型能够借助图像分类数据学习判别性更强的视觉表征，同时从图文数据对中对齐更丰富的语义概念，显著增强了视觉表征的学习效果。

\paragraph{通过分布式计算进行实现上的优化} 在iCLIP方法中，每个图像类别描述对应的语言表征由语言模型动态生成，替代了直接优化分类器权重$W$的传统做法。当图像分类数据集中的类别数量$N$较小时，语言模型的计算开销较小。
但是，对于图像类别数量$N$较大的数据集，例如包含2万多个类别的ImageNet数据集全集，在单次随机梯度下降过程中语言模型需要提取$N$个图像类别描述对应的语言表征，对训练速度产生了较大影响。
为了使iCLIP方法适用于尽可能多的图像类别，本节参考数据并行方法\cite{chen2020simple}，实现了分布式训练策略来降低这部分计算开销。
具体来说，iCLIP方法将$N$个图像类别描述均匀地分布在$G$个数据并行的模型实例上，使得每个模型实例只需要提取$N/G$个图像类别描述对应的语言表征。在计算公式\eqref{eq:iclip-unified}中的损失函数时，每个模型实例会从其他实例的输出中收集对应类别的语言表征以构建完整的对比对象集合。
这种分布式计算方法显著降低了语言模型处理图像类别描述的计算成本，并节约了$(G-1)/G$的显存开销，使得iCLIP方法能够处理图像类别数量较大的分类数据集。

\section{实验结果}
\label{sec:iclip-result}
本节将有效融合的预训练方法iCLIP，与单方法预训练基线和简单融合的多任务预训练基线进行比较，证明了引入高质量图像分类数据后对CLIP方法的增强效果。
本节分别在百万、千万和亿级别数据规模上对iCLIP方法进行了验证，表明iCLIP方法在不同的数据规模下均能表现出更优性能，并在小规模数据集上进行详细的消融实验。
为比较不同方法视觉表征学习效果，本节采用零样本开放集合图像识别任务和零样本图文跨模态检索任务对各个方法进行评测，反映了各个方法视觉-语言表征对齐水平和视觉表征对语义概念的覆盖程度。与此同时，本节还在各类视觉下游任务上验证了不同预训练方法的迁移效果，包括语义分割任务、长尾类别目标检测任务和视频动作识别任务。

\subsection{实验设置}
\label{sec:iclip-exp-setting}
本节首先详细说明预训练过程中使用的数据集和实验设置，以及评测方法。

\paragraph{预训练实验数据和设置} 本节使用百万、千万和亿级别三种不同规模的数据集组合进行预训练实验，以充分比较不同方法在不同规模下的表现。
\begin{itemize}
    \item 在百万级图像规模的实验设置中，本节选用常用的ImageNet-1K子集\cite{deng2009imagenet}（IN-1K）和Conceptual Caption 3M\cite{sharma-etal-2018-conceptual}（CC-3M）数据集作为预训练数据。作为图像分类数据，IN-1K数据集大约有128万张图片，并包含1000种不同的图像类别，而CC-3M数据集是图文数据集，大约有300万张图片和配对文本。
    在模型结构方面，视觉模型采用约有2800万个参数的Swin-Tiny\cite{Swin}结构，并使用224x224像素的图片作为输入，而语言模型则采用约有1亿2500万个参数的BERT-Base\cite{BERT}结构，并最多使用77个文本标记作为输入。视觉模型和语言模型分别通过视觉自监督方法MoBY\cite{MoBY}和语言自监督方法RoBERTa\cite{liu2019roberta}进行初始化，从而加速实验收敛速度，节约预训练的计算开销。
    在每一个批大小为$\mathcal{B}$的训练批次中，iCLIP方法分别从两个数据集采样一半的图像，因此两个数据源的总训练样本数相同。每个模型实例的批大小为128张图像，并使用数据并行方法在8个模型实例上训练了100轮。默认的学习率调度为余弦调度方法，其中前5轮会进行学习率预热至2e-4的最高学习率。在正则化方面，所有实验的权重衰减系数为0.01，并使用随机增强\cite{cubuk2020randaugment}的图像数据增强方法和比例为0.1的随机深度\cite{huang2016deep}方法增强模型的泛化性。
    \item 在千万级图像规模的实验设置中，本节选用ImageNet全集（IN-22K）和YFCC 14M\cite{YFCC100M}（YFCC-14M）数据集作为预训练数据。作为图像分类数据，IN-22K数据集大约有1400万张图片，并包含2万多种不同的图像类别，而YFCC-14M数据集是YFCC图文数据集\cite{YFCC100M}的常用子集，同样包含大约1400万张图片。相比于前一组预训练数据设置，该组实验数据规模扩大了一个数量级，以进一步验证iCLIP方法在更大规模数据上的收益。
    在模型结构方面，视觉模型同样使用Swin-Tiny结构，而语言模型则和OpenAI CLIP模型\cite{radford2021learning}相同，使用一个约有6300万个参数的Transformer\cite{Transformer}模型结构。本组实验中的视觉和语言模型均从随机初始化开始训练，以便与一些相关工作进行公平的比较。
    在该组实验中单模型实例的批大小为512张图像，并在16个模型实例上并行训练了32轮。由于该组实验预训练长度增加，需要更强的正则化方法防止模型过拟合，因此该组实验的权重衰减系数增大为0.05，而其他正则化设置和实验设置与之前相同。% 最大学习率设为2e-4或8e-4。
    此外，本组实验还包括两个数据规模相当的实验设置：1）从IN-22K数据集中去除属于IN-1K数据集类别的所有数据，构造新的图像分类数据集IN-21K，用于评估IN-1K数据集上零样本图像识别效果；2）从IN-21K数据集和YFCC-14M数据集中各采样一半数据进行组合，确保数据规模和训练开销与基线方法相同。
    \item 在亿级别图像规模的实验设置中，本节选用ImageNet全集和Laion-400M数据集\cite{schuhmann2021laion400m}作为预训练数据。Laion-400M数据集是一个从Common Crawl开源项目\cite{cc}中收集的超大规模图文数据集，包含大约4亿张来源各异、内容丰富的图像，可以充分验证iCLIP方法在超大规模数据组合下的收益。
    在该组实验设置中，视觉模型采用约有8900万个参数的Swin-Base\cite{Swin}结构，而语言模型使用BERT-Base结构，并采用自监督方法对模型进行初始化。
    整个实验共训练10万个批次，其中单模型实例的批大小为192张图像，并采用64个数据并行的模型实例同时训练。和前述实验设置略有不同，本组实验在每个训练批次中，分别从IN-22K数据集和Laion-400M数据集中采样64张和128张图像。因此，本组实验在图像分类数据集上训练约30轮，在图文数据集上训练约2轮。
    在实验设置方面，本组实验同样使用余弦学习率调度方法，并使用前16700个训练批次进行学习率预热直到1e-3的最高学习率。在正则化设置方面，模型的权重衰减系数设置为0.05，随机深度比例设置为0.2，其他设置不变。
\end{itemize}

\paragraph{评测数据集和迁移设置} 本节通过两种评测任务比较了不同预训练方法的视觉-语言表征对齐效果，并在一些下游视觉任务中对不同预训练方法进行迁移。
\begin{itemize}
    \item 零样本开放集合图像识别任务是CLIP方法的重要评测方式之一。本节在三个评测集上对比了不同预训练方法视觉表征的语义丰富度和泛化能力：1）IN-1K评测集及其变体IN-S评测集\cite{wang2019learningSketch}上的Top-1准确率，衡量了视觉表征的通用图像识别能力和鲁棒性；2）广泛使用的Kornblith-12评测集\cite{imagnettransfer}上的平均Top-1准确率，涵盖了12个不同领域的开放集合精细图像识别任务；3）UniCL\cite{unicl}方法中使用的14个开放集合精细图像识别任务的平均Top-1准确率，合称为UniCL-14泛化评测集。
    \item 领域内图像识别任务是一种特殊的评测方式。由于一些预训练设置中包含了IN-1K数据集覆盖的类别，因此在这些情况下IN-1K和IN-S评测集上的测试结果反映了不同方法的领域内图像识别能力，而不涉及开放集合识别能力。
    \item 零样本图文跨模态检索任务直接反映了CLIP方法的视觉-语言表征对齐效果。本节使用包含1000张图片的Flickr30K\cite{young2014flickr}（Flickr）评测集和包含5000张图片的MSCOCO\cite{chen2015microsoft}（COCO）评测集上对不同预训练方式进行比较，并展示不同方法在图像检索（IR）和文本检索（TR）子任务上的Top-1召回率。
    \item 在下游视觉任务上的迁移表现反映了不同预训练方法的泛化能力。本节首先对比了不同方法在少样本图像识别任务上的迁移表现。该任务使用Kornblith-12数据集上的训练集对模型进行迁移，分别使用了1到16个不等的训练样本。该任务的迁移设置与OpenAI CLIP工作\cite{radford2021learning}的设置相同：在迁移过程中固定视觉模型不变，只微调额外的类别分类器，并报告平均Top-1准确率。
    其次，本节还在语义分割、长尾类别目标检测和视频动作识别等任务上对不同预训练方法进行了迁移，并分别比较了不同方法在这些任务上的mIoU得分、检测框mAP得分和Top-1准确率。具体的迁移设置如下。针对语义分割任务，本节使用MaskFormer\cite{maskformer}方法作为迁移方法并在ADE20K数据集\cite{zhou2019ade}上进行实验。该任务中视觉模型的窗口大小设为7，其他设置与MaskFormer\cite{maskformer}方法的默认设置一致，并展示了单尺度测试结果。对于长尾类别目标检测任务，本节使用了包含1203个物体类别的LVIS数据集\cite{gupta2019lvis}，并采用Faster R-CNN\cite{ren2016faster}方法作为迁移方法。该任务迁移设置与主流方法\cite{Swin}一致，包括24轮的训练数据、图像短边长度在480至800像素之间的多尺度训练，并比较单尺度测试结果。对于视频动作识别任务，本节采用Swin方法\cite{Swin}中的迁移方法和实验设置，在Kinetics-400数据集\cite{kay2017kinetics}上训练了30轮，并报告Top-1准确率。
\end{itemize}

\subsection{百万级图像规模的组合实验}

\paragraph{从对比学习角度重新理解图像分类方法的消融实验}
表\ref{tab:iclip-ablate_head}首先分析了重新理解的图像分类方法对领域内图像识别能力的影响，因此该组实验只使用了IN-1K图像分类数据集，并报告IN-1K评测集上的Top-1准确率。第\ref{sec:iclip-adapt-classification}节和第\ref{sec:iclip-adapt-external}节中分别提出对齐图像分类损失函数方法、对齐分类器类型方法和对齐标注语义丰富程度方法。该组实验对这些方法分别进行了消融实验。% 本组实验使用IN-1K评测集Top-1的准确率反映图像分类任务本身的效果变化。
对比序号\#1与\#2的实验结果可以看出基于余弦相似度的图像分类损失函数在领域内图像识别任务上的性能略好于传统基于内积相似度的线性损失函数，因此在IN-1K评测集上有0.6\%的小幅提升。进一步引入对齐分类器类型和对齐标注语义丰富程度的方法后，图像分类任务本身的训练效果基本不变。与传统图像分类方法相比，新图像分类方法在IN-1K评测集上有0.5\%的小幅增益。
这组实验结果表明从对比学习角度重新理解图像分类方法是可行的，为iCLIP方法融合两类数据的策略提供有利支撑。

\begin{table}
  \centering
  % \addtolength{\tabcolsep}{+1.0pt}
\caption{从对比学习角度重新理解图像分类方法的消融实验}
  \begin{tabular}{lcccc}
    \toprule
    序号 & 损失函数对齐 & 分类器对齐 & 标注语义对齐 & IN-1K Top-1准确率（\%） \\
    %\hline
    % \cmidrule(lr){1-1}\cmidrule(lr){2-4}
    % \cmidrule(lr){5-5}
    \midrule
    \#1 & & & & 80.9  \\
    \#2 & $\checkmark$ & & & \textbf{81.5} \\
    \#3 & $\checkmark$ & $\checkmark$ & & 81.2  \\
    \#4 & $\checkmark$ & $\checkmark$ & $\checkmark$ & 81.4 \\
    \bottomrule
  \end{tabular}
  \label{tab:iclip-ablate_head}
\end{table}

\begin{table}
  \centering
\caption{iCLIP方法与单方法和简单融合方法基线的对比实验}
% 对于仅使用 IN-1K 的模型，我们训练它们 100 个 epoch。对于仅使用 GCC-3M 的模型，我们使用与 IN-1K 中使用的相同的迭代和批量大小来训练它们。
  \begin{tabular}{lcccc}
    \toprule
    \multicolumn{1}{c}{} &
    \multicolumn{1}{c}{} & 
    \multicolumn{1}{c}{Kornblith-12评测集}
    & \multicolumn{2}{c}{IN-1K和IN-S评测集} \\
    % \cmidrule(lr){1-1}\cmidrule(lr){2-2}
    % \cmidrule(lr){3-3}\cmidrule(lr){4-5}
    \midrule
    序号 & 方法 & 平均Top-1准确率（\%） & \multicolumn{2}{c}{Top-1准确率（\%）} \\
    %\hline
    % \cmidrule(lr){1-1}\cmidrule(lr){2-2}
    % \cmidrule(lr){3-3}\cmidrule(lr){4-5}
    \midrule
    \#1 & 仅图像分类方法 & - & \textbf{80.9} & 29.4 \\
    \#2 & 仅对比学习方法 & 31.4 & 32.4 & 18.3  \\
    % 3 & \emph{Split head (Sup.)} & - & 50.7 & 80.6 & 38.3 \\
    % 4 & \emph{Split head (Text)} & 35.1 & 54.9 & 45.0 & 24.7 \\
    \#3 & 简单融合基线 & 35.1 & 80.6 & 38.3 \\
    \#4 & iCLIP-NoD & 37.7 & 80.5 & 38.6 \\
    \#5 & iCLIP & \textbf{39.1} & 80.4 & \textbf{38.7} \\
    \bottomrule
  \end{tabular}
  \label{tab:iclip-ablate_cc}
\end{table}

\paragraph{在CC-3M图文数据集中引入IN-1K图像分类数据进行增强} 本组实验进一步探究了iCLIP方法相比于单方法预训练基线和简单融合的多任务预训练基线的性能提升。
其中简单融合基线是一种多任务预训练方法。该基线使用共享的视觉模型提取视觉表征，但保留独立的直接参数化类别分类器和语言模型分别处理图像分类数据和CLIP方法的图文数据对。因此该基线只是将公式\eqref{eq:iclip-classification}与公式\eqref{eq:iclip-clip}中介绍的两个损失函数进行平均。
实验结果如表\ref{tab:iclip-ablate_cc}所示。与序号为\#1和\#2的单方法预训练基线相比，序号\#3对应的简单融合基线已经在一定程度上提升了模型零样本开放集合图像识别能力，在Kornblith-12评测集上将平均Top-1准确率提升了3.7\%。此外，这种做法也保持了预训练方法在领域内图像识别任务上的表现，并增强了视觉表征的鲁棒性。

为了进一步分析外部专家知识库增强图像类别语义方法对预训练的收益，本组实验构造了在iCLIP方法中去除这一做法的基线，称为iCLIP-NoD。对比序号\#3对应的简单融合基线与序号\#4对应的iCLIP-NoD基线的实验结果，两种训练方法有效融合后的iCLIP-NoD基线在Kornblith-12评测集上有2.6\%的准确率提升，且在IN-1K评测集及其变体上表现相当，说明引入语言模型重参数化分类器权重能有效融合两种预训练方法。
将iCLIP-NoD基线与序号为\#1和\#2的单方法预训练基线对比，可以观察到有效融合后的方法在领域内图像识别任务上的能力没有明显变化，但在零样本开放集合图像识别任务上有6.3\%的平均Top-1准确率提升。这一结果说明在CLIP方法中引入高质量图像分类数据，可以显著增强视觉表征的语义覆盖度，提高其对开放集合类别的图像识别能力。
% 同时将其与简单融合基线\#3相比，在 Kornblith-12数据集零样本物体识别任务上的平均Top-1准确率高出 2.6\%，而它们在IN-1K评测集及其变体上表现相当。这一结果说明iCLIP方法将图像分类任务重构并与实例级语言监督任务有效融合，能更好地汇聚两种学习方案的优点。
此外，引入外部专家知识库增强图像类别语义之后，序号\#5对应的iCLIP方法在Kornblith-12数据集上得到进一步改进，并将平均Top-1准确率提升了1.4\%。这一结果说明对齐不同任务的标注信息语义丰富度有助于模型区分细粒度概念，增强其图像识别能力。


\subsection{千万级图像规模的组合实验}
本组实验将数据规模扩展了一个数量级，以进一步验证iCLIP方法在更大规模和类别数更多的数据组合上的性能表现。

\begin{table}
    \centering
    \caption{引入图像分类数据增强CLIP方法图像识别能力的实验}
    % 模型从头开始预训练的 32 个轮次。COCO 和 Flickr 代表 MSCOCO [31] 和 Flickr30K [64]。IR 和 TR 代表图像检索和文本检索。
    \begin{tabular}{lcccc}
    \toprule
        \multicolumn{1}{c}{} &
        \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{2}{c}{领域内和零样本开放集合图像识别任务} \\
        % \cmidrule(lr){1-1}
        % \cmidrule(lr){2-3}
        % \cmidrule(lr){4-5}
        \midrule
        序号 & 数据 & 方法 & IN-1K评测集（\%） & UniCL-14泛化评测集（\%）  \\
        % \cmidrule(lr){1-1}
        % \cmidrule(lr){2-3}
        % \cmidrule(lr){4-5}
        \midrule
        % ImageNet-1K &  Swin-Tiny \\
        \#1 & YFCC-14M & CLIP & 30.1 & 36.3 \\

        \#2 & 数据组合一 & iCLIP-NoD & 39.4 & 45.4 \\  
                
        \#3 & 数据组合一 & iCLIP & \textbf{45.9} & \textbf{49.9} \\    
        
        % \cmidrule(lr){1-1}
        % \cmidrule(lr){2-3}
        % \cmidrule(lr){4-5}
        \midrule
        
        \#4 & 数据组合二 & iCLIP-NoD & 41.1 & 49.4  \\  
                
        \#5 & 数据组合二 & iCLIP & \textbf{50.9} & \textbf{54.4} \\   
        
        % \cmidrule(lr){1-1}
        % \cmidrule(lr){2-3}
        % \cmidrule(lr){4-5}
        \midrule        
        
        \#6 & 数据组合三 & iCLIP-NoD & 76.2 & 51.6 \\
        
        \#7 & 数据组合三 & iCLIP & \textbf{76.3} & \textbf{55.5} \\

        \bottomrule
    \end{tabular}
    % since official codebase doesn't support training.
    % top-1 acc for zero-shot recognition; rank@1 for cross-modal retrieval
    \label{tab:iclip-ablate_yfcc}
\end{table}

\paragraph{在YFCC-14M图文数据集中引入IN-22K图像分类数据进行增强} 与前述实验类似，本组实验也比较了引入高质量图像分类数据前后对CLIP方法预训练效果的影响。在本组实验中，除了使用领域内和零样本开放集合图像识别任务对不同预训练方法进行评测，还比较了不同方法零样本图文跨模态检索能力，展示了不同方法视觉-语言表征的对齐程度。
如第\ref{sec:iclip-exp-setting}节所述，本组实验中包括三种数据组合变体：1）组合一：IN-21K和YFCC-14M数据集各半，以评测iCLIP方法的数据利用效率；2）组合二：完整IN-21K和YFCC-14M数据集组合，以评测IN-1K评测集上的零样本图像识别能力；3）组合三：IN-22K和YFCC-14M数据集组合，以评测IN-1K评测集上的领域内图像识别能力。

表\ref{tab:iclip-ablate_yfcc}展示了不同方法和数据组合方式在领域内和零样本开放集合图像识别任务上的评测结果。与序号\#1对应的单CLIP方法基线相比，引入图像分类数据进行增强的iCLIP-NoD方法，在数据规模相当的情况下，在IN-1K评测集的零样本图像识别任务上取得了9.3\%的Top-1准确率提升，在UniCL-14泛化评测集的零样本开放集合图像识别任务上也有9.1\%的平均Top-1准确率提升。这一结果充分说明了iCLIP方法通过引入高质量图像分类数据的方式，显著提升了CLIP方法的数据利用效率，并增强了视觉表征的语义信息建模能力。

% 中的\#2、\#4和\#6三组结果展示了三种不同的数据集组合设置下iCLIP预训练方法的评测结果。与实例级语言监督方法——CLIP基线（\#1）相比，在使用数据量完全公平的实验组\#2下，iCLIP在IN-1K评测集上的零样本识别能力有8.3\%的较大收益，在14个细粒度泛化评测数据集上进行零样本物体识别效果上也有9.1\%的平均Top-1准确率提升。
\begin{table}
    \centering
    \caption{引入图像分类数据增强CLIP方法图文跨模态检索能力的实验}
    % 模型从头开始预训练的 32 个轮次。COCO 和 Flickr 代表 MSCOCO [31] 和 Flickr30K [64]。IR 和 TR 代表图像检索和文本检索。
    \begin{tabular}{lcccccc}
    \toprule
        \multicolumn{1}{c}{} &
        \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{4}{c}{零样本图文跨模态检索任务（\%）} \\
        % \cmidrule(lr){1-1}
        % \cmidrule(lr){2-3}
        % \cmidrule(lr){4-7}
        \midrule
        序号 & 数据 & 方法 & Flickr-IR & Flickr-TR & COCO-IR & COCO-TR    \\
        % \cmidrule(lr){1-1}
        % \cmidrule(lr){2-3}
        % \cmidrule(lr){4-7}
        \midrule
        % ImageNet-1K &  Swin-Tiny \\
        \#1 & YFCC-14M & CLIP  & 21.5 & 37.9 & 12.5 & 21.2  \\

        \#2 & 数据组合一 & iCLIP-NoD  & 27.6 & 39.1 & 13.0 & 20.4 \\  
                
        \#3 & 数据组合一 & iCLIP & \textbf{31.9} & \textbf{49.8} & \textbf{15.5} & \textbf{27.2} \\    
        
        % \cmidrule(lr){1-1}
        % \cmidrule(lr){2-3}
        % \cmidrule(lr){4-7}
        \midrule
        
        \#4 & 数据组合二 & iCLIP-NoD & 33.4 & 51.2 & 16.3 & 26.5  \\  
                
        \#5 & 数据组合二 & iCLIP & \textbf{37.1} & \textbf{55.7} & \textbf{18.5} & \textbf{30.7} \\   
        
        % \cmidrule(lr){1-1}
        % \cmidrule(lr){2-3}
        % \cmidrule(lr){4-7}
        \midrule
        
        \#6 & 数据组合三 & iCLIP-NoD & 33.2 & 48.2 & 14.4 & 23.8 \\
        
        \#7 & 数据组合三 & iCLIP & \textbf{36.2} & \textbf{55.3} & \textbf{18.0} & \textbf{29.7} \\

        \bottomrule
    \end{tabular}
    % since official codebase doesn't support training.
    % top-1 acc for zero-shot recognition; rank@1 for cross-modal retrieval
    \label{tab:iclip-ablate_yfcc_retrieval}
\end{table}

表\ref{tab:iclip-ablate_yfcc_retrieval}展示了不同方法和数据组合方式在零样本图文跨模态检索任务上的评测结果。对比序号\#1对应的CLIP方法，序号\#2对应的iCLIP-NoD方法在相对简单的Flickr图文跨模态检索基准上有一定的性能提升，但在更复杂的COCO图文跨模态检索基准上表现相当。
这一结果表明引入高质量图像分类数据后，虽然CLIP方法的视觉表征与语言表征对齐效果得到了一定的提升，但是不对齐的标注信息语义丰富度影响了CLIP方法的语言模型学习效果，因此iCLIP-NoD方法在较难的图文跨模态检索任务上并没有取得预期的性能改进。而这一问题可以通过外部专家知识库增强方法进行有效缓解。
% 得到的新实例级语言监督预训练方法在数据利用效率方面都有较大提升。此外，考虑使用更多数据的情况，实验\#4和\#6两组在各个任务上都有进一步的提升。

\paragraph{外部专家知识库增强图像类别语义的效果} 
表\ref{tab:iclip-ablate_cc}的实验表明，外部专家知识库增强方法可以提升CLIP方法的零样本开放集合图像识别能力。但是表\ref{tab:iclip-ablate_cc}实验中使用的图像分类数据集为IN-1K数据集。这一数据集仅包含1000种不同的图像类别，且不同类别间的标签文本区分性较好，因此没有充分反映出引入外部专家知识库增强图像类别语义对模型性能的帮助。
本组实验进一步讨论了该方法在更大规模尤其是图像类别数更多的数据集上的作用。通过对比表\ref{tab:iclip-ablate_yfcc}中的三组实验（\#2与\#3、\#4与\#5、\#6与\#7）可以发现，在三种不同的数据集组合设置下使用外部专家知识库将类别标签文本扩展为图像类别描述后，CLIP方法在零样本开放集合图像识别任务上均有明显的性能提升。
具体而言，在各使用一半IN-21K和YFCC-14M数据集的数据组合一设置下，外部专家知识库的引入将CLIP方法在IN-1K评测集上的Top-1准确率提高了6.5\%，在UniCL-14泛化评测集上的平均Top-1准确率提高了4.5\%，甚至超过了使用一倍多训练数据量的数据组合二设置。
此外，表\ref{tab:iclip-ablate_yfcc_retrieval}的实验也表明外部专家知识库能够对齐不同数据集的标注信息语义丰富度，进而显著提升了CLIP方法在零样本图文跨模态检索任务上的表现。% 对比表\ref{tab:iclip-ablate_yfcc}的序号\#2与\#3，序号\#4与\#5和序号\#6与\#7的实验组可以发现，外部专家知识库增强图像类别语义方法增强了iCLIP方法在所有图文跨模态检索基准上的表现。
% 对于图像检索文本任务，该方法可以在 Flickr30K和MSCOCO评测集上分别提升10.7\%和6.8\%的Top-1召回率，在文本检索图像任务上也有2\%到4\%的提升。
% 和图文检索任务上取得一致改进。

\begin{table}
    \centering
    % \addtolength{\tabcolsep}{-1.0pt}
    \caption{不同图像分类数据和图文数据对混合比例的消融实验}
    \begin{tabular}{lccccc}
    \toprule
        % \multicolumn{1}{c}{} &
        % \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{2}{c}{Zero-shot retrieval} \\
        % \cmidrule(lr){1-1}
        % \cmidrule(lr){2-3}
        % \cmidrule(lr){4-4}
        % \cmidrule(lr){5-6}
        序号 & 训练数据混合比例 & 方法 & IN-1K & COCO-IR & COCO-TR    \\
        % \cmidrule(lr){1-1}
        % \cmidrule(lr){2-3}
        % \cmidrule(lr){4-4}
        % \cmidrule(lr){5-6}
        \midrule
        % ImageNet-1K &  Swin-Tiny \\
        \#1 & ~100\% YFCC-14M + 0\% IN-21K ~& CLIP & 30.1 & 12.5 & 21.2  \\

        \#2 & {90\% YFCC-14M + 10\% IN-21K} & iCLIP & 40.9 & 13.9 & 25.5 \\ 

        \#3 & {75\% YFCC-14M + 25\% IN-21K} & iCLIP & 43.9 & 15.2 & \textbf{27.5} \\ 
        
        \#4 & 50\% YFCC-14M + 50\% IN-21K & iCLIP & \textbf{45.9} & \textbf{15.5} & 27.2 \\    
        
        \#5 & {25\% YFCC-14M + 75\% IN-21K} & iCLIP & 44.8 & 14.1 & 27.1 \\  

        \#6 & {10\% YFCC-14M + 90\% IN-21K} & iCLIP & 44.2 & 14.9 & 25.9 \\
        \bottomrule
    \end{tabular}
    \caption*{IN-1K评测集汇报了Top-1准确率（\%）而COCO评测集汇报了Top-1召回率（\%）}
    % since official codebase doesn't support training.
    % top-1 acc for zero-shot recognition; rank@1 for cross-modal retrieval
    \label{tab:iclip-ablate_combine}
\end{table}

\paragraph{混合不同比例图像分类数据的消融实验} 如第\ref{sec:iclip-exp-setting}节所述，本组实验的图像分类数据和图文数据对混合比例为各50\%。
由于不同数据集的规模可能不同，因此为了验证iCLIP方法的推广性，表\ref{tab:iclip-ablate_combine}中讨论了YFCC-14M图文数据集与IN-21K图像分类数据集不同混合比例下的模型在零样本开放集合图像识别和图文跨模态检索任务上的表现，其中序号\#4对应的实验为默认数据比例设置。
从表\ref{tab:iclip-ablate_combine}中的结果可以看出，在不同的混合比例下，引入高质量图像分类数据后的iCLIP方法相比于CLIP方法，在图像识别和图文检索任务上都有较大提升。其中，仅引入10\%的图像分类数据就对CLIP方法的预训练效果带来显著提升，在零样本开放集合图像识别任务上取得10.8\%的Top-1准确率改善。当两种数据混合比例为各50\%时，iCLIP方法达到最佳性能。
% 特别地考虑海量级的（图像，替代文本）数据和规模有限的（图像，类别）数据的组合，仅引入1/10的图像分类数据就有非常显著的收益（见实验\#2）。

\paragraph{与同期工作的比较} 
与本章工作同期也有一个工作UniCL\cite{unicl}采用了类似的想法：通过对比学习思想重新理解图像分类方法，从而与CLIP方法进行有效融合，验证了本章介绍的基于高质量数据增强CLIP预训练方法的有效性。
虽然两种方法具有相似的动机，但两者在方法设计上仍有两处较大区别，影响了不同方法的融合程度和预训练效果。
% 对齐损失函数的基于余弦相似度的图像分类损失函数方法、对齐分类器类型的语言模型重参数化分类器权重方法、对齐标注语义丰富程度的外部专家知识库增强图像类别语义方法

\begin{figure}
  \centering
  \includegraphics[width=0.8\linewidth]{figures/iclip-unicl.pdf}
  \caption{CLIP、图像分类、UniCL与iCLIP方法的对比图}
  \label{fig:iclip-unicl}
\end{figure}


首先，两种方法在如何融合两种预训练任务的设计上有区别。图\ref{fig:iclip-unicl}展示了CLIP方法、图像分类方法、UniCL方法与本文介绍的iCLIP方法的异同。
UniCL方法和iCLIP方法都在图像分类方法中引入了基于余弦相似度的损失函数和语言模型重参数化分类器权重方法，这使得两种数据集的标注信息，包括图文数据对中的文本信息或图像分类数据中的类别信息，都可以通过共享的语言模型来建模，从而建立不同语义概念间的内部联系。

但UniCL方法和iCLIP方法在损失设计上并不相同。由于批大小的限制，同一批次内来自图像分类数据的图像只会覆盖$N$个图像类别中的一小部分。iCLIP方法会将这些图像与所有可能的图像类别进行对比学习，而UniCL方法只会将其与当前批次采样到的小部分图像类别进行对比学习，负样本对数量明显减小。
因此UniCL方法的损失设计很大程度上减小了图像分类数据可获取的训练信号，无法充分利用判别性好、可区分性高的图像分类数据标注信息。
% 此外，UniCL方法要求CLIP方法与图像分类的类别标签一起进行判别，由于替代文本语义涵盖更加丰富往往会涵盖类别名称，使得CLIP图文对的对比学习任务然而出现混淆情况。% 可能画个图可以说清楚
UniCL方法的损失设计优势在于训练架构更为简单。但如第\ref{sec:iclip-unify}节中所述，iCLIP方法提出了一个通过分布式计算的实现优化方案，可以将图像分类数据中涉及的所有类别均匀地分配给不同的数据并行模型实例，因此同样高效支持使用图像类别数较多的大规模图像分类数据进行训练。%并在第\ref{sec:iclip-22k-laion}节中将其推广到亿级别的训练数据。

其次，两种方法的第二个主要区别在于iCLIP方法通过引入外部专家知识库，进一步缩小了图像类别的标签文本和图文数据对的文本内容间标注语义丰富度的差异。如前文的消融实验分析所示，这种方法通过从外部专家知识库中查询与图像类别相关的额外语义信息，将类别标签扩展为内容更丰富的图像类别描述，提升了iCLIP方法视觉表征的语义丰富度，也增强了CLIP方法语言模型建模不同语义概念的能力。


\begin{table}
%\vspace{-0.5em}
    \centering
    % \addtolength{\tabcolsep}{-1.0pt}
    \caption{iCLIP方法与UniCL方法在图像识别能力上的对比实验}
    \begin{tabular}{lcccc}
    \toprule
        \multicolumn{1}{c}{} &
        \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{2}{c}{领域内和零样本开放集合图像识别任务} \\
        % \cmidrule(lr){1-1}
        % \cmidrule(lr){2-3}
        % \cmidrule(lr){4-5}
        \midrule
        序号 & 数据 & 方法 & IN-1K评测集（\%） & UniCL-14泛化评测集（\%）   \\
        % \cmidrule(lr){1-1}
        % \cmidrule(lr){2-3}
        % \cmidrule(lr){4-5}
        \midrule
        % ImageNet-1K &  Swin-Tiny \\
        \#1 & 数据组合一 & UniCL & 36.4 & 45.5 \\  
        \#2 & 数据组合一 & iCLIP & \textbf{45.9} & \textbf{49.9} \\  

        % \cmidrule(lr){1-1}
        % \cmidrule(lr){2-3}
        % \cmidrule(lr){4-5}
        \midrule
        
        \#3 & 数据组合二 & UniCL & 40.5 & 49.1 \\
        \#4 & 数据组合二 & iCLIP & \textbf{50.9} & \textbf{54.4} \\   

        % \cmidrule(lr){1-1}
        % \cmidrule(lr){2-3}
        % \cmidrule(lr){4-5}
        \midrule
        
        \#5 & 数据组合三 & UniCL & 70.5  & 52.4 \\
        \#6 & 数据组合三 & iCLIP & \textbf{76.3} & \textbf{55.5} \\

        \bottomrule
    \end{tabular}
    \label{tab:iclip-tounicl}
\end{table}

表\ref{tab:iclip-tounicl}展示了UniCL方法和iCLIP方法在IN-1K评测集和UniCL-14泛化评测集上的Top-1准确率，以对比两种方法在领域内和零样本开放集合图像识别任务上的表现。在三种不同的数据集组合设置下，iCLIP方法在IN-1K评测集上的Top-1准确率比UniCL方法至少高出5\%。具体而言，在数据组合三的实验中，由于预训练数据包含IN-1K数据集的图像类别，此时IN-1K评测集的结果反映了预训练方法的领域内图像识别能力。在该项任务上iCLIP方法仍明显优于UniCL方法，表明UniCL方法的损失设计并没有充分利用图像分类数据的训练信号。同时，在UniCL-14泛化评测集代表的零样本开放集合图像识别任务上，iCLIP方法也明显优于UniCL方法。这些实验结果表明，在图像分类数据中使用完整类别集合进行对比学习能够显著增强预训练视觉表征的判别能力。
% 同时在最大规模的IN-22K与YFCC-14M组合数据上，如实验序号\#6所示，iCLIP方法在泛化评测集上达到55.5\%的平均Top-1 准确率，比实验序号\#5的UniCL方法高3.1\%。
此外，表\ref{tab:iclip-tounicl-ret}中比较了UniCL方法和iCLIP方法在零样本图文跨模态检索任务上的性能，并展示了Flickr数据集和COCO数据集上图像检索与文本检索任务的Top-1召回率。得益于第\ref{sec:iclip-adapt-external}节中提出的外部专家知识库增强图像类别语义做法，iCLIP方法在各个图文跨模态检索测试中均取得优于UniCL方法的表现。



\begin{table}
%\vspace{-0.5em}
    \centering
    % \addtolength{\tabcolsep}{-1.0pt}
    \caption{iCLIP方法与UniCL方法在图文跨模态检索能力上的对比实验}
    \begin{tabular}{lcccccc}
    \toprule
        \multicolumn{1}{c}{} &
        \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{4}{c}{零样本图文跨模态检索任务（\%）} \\
        % \cmidrule(lr){1-1}
        % \cmidrule(lr){2-3}
        % \cmidrule(lr){4-7}
        \midrule
        序号 & 数据 & 方法 & Flickr-IR & Flickr-TR & COCO-IR & COCO-TR    \\
        % \cmidrule(lr){1-1}
        % \cmidrule(lr){2-3}
        % \cmidrule(lr){4-7}
        \midrule
        % ImageNet-1K &  Swin-Tiny \\
        \#1 & 数据组合一 & UniCL & 21.5 & 37.9 & 12.5 & 21.2 \\  
        \#2 & 数据组合一 & iCLIP & \textbf{31.9} & \textbf{49.8} & \textbf{15.5} & \textbf{27.2} \\  

        % \cmidrule(lr){1-1}
        % \cmidrule(lr){2-3}
        % \cmidrule(lr){4-7}
        \midrule
        
        \#3 & 数据组合二 & UniCL & 34.0 & 50.3 & 17.7 & 28.0 \\
        \#4 & 数据组合二 & iCLIP & \textbf{37.1} & \textbf{55.7} & \textbf{18.5} & \textbf{30.7}\\   
        % \cmidrule(lr){1-1}
        % \cmidrule(lr){2-3}
        % \cmidrule(lr){4-7}
        
        % 5 & YFCC + IN-22K & UniCL~\cite{unicl} & -  & - & - & - \\
        % 6 & YFCC + IN-22K & iCLIP & 36.2 & 55.3 & 18.0 & 29.7 \\
        \bottomrule
    \end{tabular}

    \label{tab:iclip-tounicl-ret}
\end{table}


%$^\ddag$表示为本文复现结果。因为使用IN-22K与YFCC-14M组合数据训练的UniCL模型并未公开，这一数据组合在此任务上不进行汇报。


\begin{table}
  \centering
  % \addtolength{\tabcolsep}{-2.0pt}
    \caption{iCLIP方法与其他应用CLIP方法的工作的对比实验}
    %IN-22K和Laion-400M数据组合上的消融研究。我们在 ImageNet 数据集 （IN-1K [8] 和 IN-S [56]）上评测模型，并在 Kornblith-12 数据集基准 [27] 上评测零样本评测。在 Kornblith-12 数据集上进行了小样本学习和对三个下游任务的微调，以评测iCLIP的迁移能力。$^\ddag$表示使用公开的模型权重进行测试。}
  \begin{tabular}{lcccccc}
  % method | visual encoder arch | image numbers# | IN-1K | IN-S | 12-zero | 12-four-shot | ADE | LVIS | K400
    \toprule
    \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{2}{c}{IN-1K和IN-S评测集} &
    \multicolumn{2}{c}{Kornblith-12评测集} \\
    % \cmidrule(lr){1-1} \cmidrule(lr){2-2} \cmidrule(lr){3-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7} 
    \midrule
    序号 & 方法 & 预训练数据 & IN-1K & IN-S & 零样本 & 少样本\\
    % \cmidrule(lr){1-1} \cmidrule(lr){2-2} \cmidrule(lr){3-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7}
    \midrule
    \#1 & OpenAI CLIP & 私有数据 & 68.6 & 46.6 & 68.8 & 66.4 \\
    
    \#2 & OpenCLIP & Laion-400M & 67.1 & 52.4 & \textbf{70.9} & -\\ %86.1+91.7+71.4+50.2+69.4+83.7+17.7+82.9+50.8+89.3+91.7+66.6 = 801.3 + 50.2
 
    % \cmidrule(lr){1-1} \cmidrule(lr){2-2} \cmidrule(lr){3-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7} 
    \midrule
    \#3 & 仅图像分类 & ImageNet全集 & 82.6 & 42.0 & - & 67.6 \\
    \#4 & 仅对比学习 & Laion-400M & 61.1 & 51.5 & 67.2 & 73.3 \\
    \multirow{2}{*}{\#5} & \multirow{2}{*}{iCLIP} & ImageNet全集 & \multirow{2}{*}{\textbf{82.9}} & \multirow{2}{*}{\textbf{59.8}} & \multirow{2}{*}{70.6} & \multirow{2}{*}{\textbf{78.1}} \\
    & & 和Laion-400M & & & & \\
    \bottomrule
  \end{tabular}
  \caption*{所有评测使用Top-1准确率（\%）且在少样本设置中每个类别使用4个训练样本}
  \label{tab:iclip-overall-zeroshot}
\end{table}



\subsection{亿级图像规模的组合实验}
\label{sec:iclip-22k-laion}
本组实验使用最大规模的公开数据集对iCLIP方法进行验证，展示了在超大数据规模下高质量图像分类数据对CLIP方法的提升效果，并与其他大规模方法进行比较。除了对比不同方法在零样本开放集合图像识别任务上的表现，本节实验还在各类下游视觉任务中对预训练模型进行迁移，比较了不同方法在少样本图像识别、语义分割、长尾类别目标检测和视频动作识别等任务上的迁移效果。

% $^\ddag$符号表示使用公开的模型权重进行测试
\paragraph{领域内和零样本开放集合图像识别任务评测结果} 表\ref{tab:iclip-overall-zeroshot}中展示了iCLIP方法与其他应用CLIP方法的工作：OpenAI CLIP\cite{radford2021learning}工作和OpenCLIP\cite{openclip}工作的对比结果，以及iCLIP方法与单方法预训练基线的对比结果。
% 对于单图像分类任务的基线，结果直接使用Swin-Base发布版本的结果，该模型在IN-22K上训练了90个轮次。对于单实例级语言监督任务的基线，结果来自在Laion-400M上预训练3个轮次的结果，因此该组实验使用的图片数目与单图像分类任务中使用的图片数目相当。
实验结果显示，iCLIP方法在IN-1K评测集上与单图像分类方法基线的结果相当，这表明两种方法在处理领域内图像识别任务时具有相当的性能。在IN-S评测集上，iCLIP方法比单图像分类方法基线提升了17.8\%的Top-1准确率，证明了iCLIP方法得到的视觉表征鲁棒性更强，对图像分布变化的建模能力更稳定。与单对比学习方法基线（也即CLIP方法基线）相比，iCLIP方法在Kornblith-12评测集上的零样本开放集合图像识别任务平均Top-1准确率提升了3.4\%。
由于预训练数据中的ImageNet全集覆盖了IN-1K数据集的图像类别，iCLIP方法在该评测集上的表现显著优于依赖私有数据或更多训练轮数的其他工作。
更重要的是，相比于OpenAI CLIP工作和OpenCLIP工作，iCLIP方法在IN-S评测集上的Top-1准确率也明显更优。这一结果证明了iCLIP方法提升了习得视觉表征的鲁棒性。
此外，iCLIP方法虽然预训练数据轮次较少，但是在Kornblith-12评测集上的平均Top-1准确率与OpenCLIP工作相当，并优于使用私有数据的OpenAI CLIP工作。 % 这也可能由于视觉模型结构有差异+Self pretrain


\begin{figure}
  \centering
  \includegraphics[width=0.8\linewidth]{figures/iclip-fewshot.pdf}
  \caption{iCLIP方法与不同方法在少样本图像识别任务上的分类性能对比}
  % todo: CLIP改为OpenAI CLIP
  % \todo{图4：与 Kornblith 12 数据集上小样本分类的 CLIP-ViT-B/16 的主要比较，展示了平均Top-1准确率。⋆表示零镜头性能。使用发布的模型再现了 CLIP 对少数镜头分类的结果。我们每个实验运行 3 次，并报告平均结果。}
  \label{fig:iclip-fewshot}
\end{figure}

\paragraph{少样本图像识别任务迁移结果} 本组实验还比较了不同预训练方法在少样本图像识别任务上的迁移性能，并选用Kornblith-12数据集作为训练和评测数据集。这项迁移任务的实验设置遵循了OpenAI CLIP工作的做法：在迁移过程中固定视觉模型不变，只微调额外的类别分类器。
% 这组评测按照CLIP给出的设置实现：冻结视觉模型并增加一个额外的线性分类层以进行少样本微调。
如图\ref{fig:iclip-fewshot}所示，OpenAI CLIP工作在每个图像类别可训练样本数少于4个时的迁移表现要弱于零样本评测结果。这是因为当少样本图像识别任务的可训练样本数量有限时，随机初始化的类别分类器难以得到很好的训练，因此此时的类别分类器无法充分反映模型视觉表征中蕴含的语义信息。
基于前述利用对比学习方法重新表达图像分类任务的方法，本组实验提出直接微调经过预训练的语言模型作为类别分类器，而不是微调随机初始化的类别分类器。预训练过的语言模型能够为少样本图像识别任务迁移提供良好的初始化，并缩小了预训练阶段与少样本图像识别任务迁移阶段之间的差异。

当每个图像类别只使用1个训练样本时，这种语言模型重参数化分类器的迁移方法使得iCLIP方法达到73.9\%的平均Top-1准确率，相比于零样本基线高出3.3\%，而相比于OpenAI CLIP工作高出29.5\%。这一结果证明了语言模型重参数化分类器的迁移方法在少样本图像识别任务上更具优势。
当每个图像类别使用16个训练样本时，iCLIP方法的迁移性能仍然比OpenAI CLIP工作要高4.1\%。
与单图像分类方法和单CLIP方法的基线相比，iCLIP方法在单样本迁移下的平均Top-1准确率分别提高了24.6\%和6.1\%，并在16个训练样本的设置中依然保持2.7\%和5.0\%的性能提升。

\begin{table}
  \centering
  % \addtolength{\tabcolsep}{-2.0pt}
    \caption{iCLIP方法与不同方法在下游视觉任务上的迁移性能比较实验}
    %IN-22K和Laion-400M数据组合上的消融研究。我们在 ImageNet 数据集 （IN-1K [8] 和 IN-S [56]）上评测模型，并在 Kornblith-12 数据集基准 [27] 上评测零样本评测。在 Kornblith-12 数据集上进行了小样本学习和对三个下游任务的微调，以评测iCLIP的迁移能力。$^\ddag$表示使用公开的模型权重进行测试。}
  \begin{tabular}{lccc}
  % method | visual encoder arch | image numbers# | IN-1K | IN-S | 12-zero | 12-four-shot | ADE | LVIS | K400
    \toprule
    \multicolumn{1}{c}{} & \multicolumn{1}{c}{语义分割} & \multicolumn{1}{c}{长尾类别目标检测} & \multicolumn{1}{c}{视频动作识别}\\
    % \cmidrule(lr){1-1} \cmidrule(lr){2-4}
    \midrule
  方法 & mIoU得分 & 检测框mAP得分 & Top-1准确率（\%）\\
    % \cmidrule(lr){1-1} \cmidrule(lr){2-4}
    \midrule
    仅图像分类方法 & 52.1 & 35.9 & 82.7 \\
    仅对比学习方法 & 52.0 & 36.6 & 82.3 \\
    iCLIP &\textbf{52.6} & \textbf{37.9} & \textbf{83.1} \\
    \bottomrule
  \end{tabular}

  \label{tab:iclip-overall-transfer}
\end{table}

\paragraph{在更多视觉下游任务上的迁移结果} 本节对比了不同方法在更多视觉下游任务上的迁移效果。如表\ref{tab:iclip-overall-transfer}所示，与单图像分类方法的基线相比，iCLIP方法在语义分割、长尾类别目标检测和视频动作识别任务上分别提升了0.5的mIoU得分、2.0的检测框mAP得分和0.4\%的Top-1准确率。与单对比学习方法的基线（也即CLIP方法基线）相比，iCLIP方法在三项视觉任务上的迁移结果也分别提高了0.6、1.3和0.8\%。这些结果表明iCLIP方法获得的视觉表征具有更强的泛化能力。


\section{总结}
\label{sec:iclip-summary}
% 我们的贡献总结如下：
% ・我们将图像分类和对比语言 - 图像预训练这两个重要的视觉任务合并到一个框架中。
% ・我们发现原始图像分类公式可以适应 CLIP 方法，而性能几乎没有下降。根据这一发现，我们提出了一种有效融合方法，其中两个任务共享相同的文本编码器和相同的分类器类型，其有效性在基准测试中得到了广泛验证。
% ・我们提出了一种简单而有效的方法，将知识库引入图像分类中，解决了原始短图像名称的歧义和多义问题，并进一步弥合了类和替代文本之间的差距。它还首次展示了将知识库应用于计算机视觉问题。
% 背景+motivation
语言-图像对比学习方法通过对比学习框架，在图文数据对中构建正负样本对来驱动模型预训练，并将训练数据扩展到大规模互联网图文数据对。但是这些图文数据对中存在大量噪声数据，影响了CLIP方法视觉表征与语言表征的对齐效果。

% 解决方案和发现
本章提出借助已有的人工标注数据（如高质量图像分类数据）对互联网图文数据对进行补充。同时构建了iCLIP预训练方法，从而实现两种不同形式的数据源及其对应的训练方法的有效融合。iCLIP方法一方面从对比学习角度重新理解传统的图像分类方法，统一了两种方法的损失函数形式和分类器参数化方式，另一方面通过引入外部专家知识库来增强图像类别语义信息，有效地弥合了不同数据在标注信息语义丰富度上的差距，并减少了不同类别标签间的歧义性。
% 引入已有经过人工标注的数据源进行增强，一方面扩展了数据来源和监督信号类别，另一方面也提高了数据整体信噪比，有利于数据利用效率的提升。特别地，本章以已有的图像分类数据为主要对象，经过对图像分类任务的解析与重构，发现该类任务可以从实例级语言监督任务的角度重新诠释和建模，且不会损害其原有的学习效率，因此使得其有机会与原先的基于实例级对比的联合预训练方法，也即CLIP，进行有效融合。
% 具体来说，为将传统的图像分类任务与基于实例级对比的联合预训练方法对齐，引入了三项调整内容：用余弦相似度统一损失函数，用语言模型统一监督信号建模，引入外部知识哭缩小标签信息粒度差距。
采用高质量图像分类数据增强的iCLIP方法，在零样本开放集合图像识别任务和图文跨模态检索任务上取得明显性能提升。该方法的有效性已在亿级规模的训练数据上得到验证。
% 同时本章工作在三种不同的数据组合规模，从百万级到亿级图片上进行了验证，充分展示了该方法的数据可扩展性。

% 引入下一章
虽然iCLIP方法在语义分割、目标检测等下游视觉任务上的迁移性能有所提升，但提升的幅度相对有限。因此，下一章将深入研究CLIP方法在细粒度视觉任务上的迁移性能，并提出相应的改进方案。