% !TeX root = ../main.tex
\chapter{总结与展望}

\section{本文总结}
“预训练-微调”范式作为数据驱动的深度学习方法中的重要范式，在计算机视觉和自然语言处理领域取得了显著成功。该范式通过解耦通用表征学习与下游任务迁移两个阶段，利用大规模预训练数据缓解下游任务数据标注难的问题，从而提升了模型迁移性能。视觉任务的核心挑战在于实现从像素级感知特征到语义级认知概念的有效映射。
传统的视觉预训练方法各有局限：有监督图像分类方法受限于封闭的类别语义空间和数据扩展性，而自监督方法则停留在感知特征层面，无法获取语义知识。
语言-图像对比学习（CLIP）方法借助语言模态在语义表达上的优势，通过大规模互联网图文对数据为开放语义的视觉表征学习提供了有效解决方案。
然而，CLIP方法仍面临着预训练数据的语义噪声、细粒度视觉任务的迁移效果欠佳以及缺乏语义生成能力等方面的挑战。
针对这些问题，本文提出了一系列创新性解决方案，显著提升了模型在视觉表征学习和下游任务迁移两个方面的性能。本文的主要创新成果为：

\begin{itemize}
    \item 针对互联网图文对数据中的语义信号噪声问题，\textit{提出基于高质量图像分类数据扩展的预训练方法。} 本文首先对比分析了互联网图文对数据的噪声特点与图像分类标注的高可区分性，提出利用高质量图像分类数据增强CLIP方法的视觉表征学习效果。通过从损失函数形式、分类器权重参数化方法、标注信息的语义丰富度三个维度分析了两种方法和数据的不同，本文提出从对比学习的视角重新构建图像分类任务的方法，实现了建模形式的统一，并引入分布式优化策略来处理类别数目较大的分类数据集。为进一步增强视觉表征的语义表达能力，本文引入外部专家知识库扩充类别标签的语义信息，实现了两种数据源的深度融合利用。该方法在多个数据组合上得到验证，显著提升了模型在零样本开放集合图像识别和图文跨模态检索任务上的性能，证明了通过对比学习框架整合高质量视觉数据来增强CLIP方法视觉-语言表征对齐效果的可行性。
    
    \item 针对CLIP模型在细粒度视觉任务上迁移效果欠佳的问题，\textit{提出基于特征图自蒸馏增强的细粒度视觉任务迁移方法。} 尽管CLIP视觉模型通过语言监督获得了丰富的语义表征，本文首先发现其在需要密集预测能力的细粒度视觉任务上的迁移表现不及像素级自监督预训练模型。针对这一问题，本文从输入完整性、训练目标粒度和损失函数设计三个维度对比分析了两类预训练方法，揭示了像素级视觉训练目标对提升模型密集预测能力的关键作用。考虑到对互联网图文对数据进行像素级标注的巨大成本和重新预训练的计算开销，本文提出特征图自蒸馏方法。该方法利用CLIP模型的输出特征图作为自蒸馏目标得到新模型：既引入了像素级视觉训练信号，又保留了原始模型中的语义信息。该方法在语义分割、目标检测、深度估计等细粒度视觉任务上显著提升了CLIP模型的迁移性能，并成功推广到其他视觉预训练模型，展现出良好的通用性。

    \item 针对CLIP模型缺乏直接完成语义生成任务的问题，\textit{提出基于离散扩散方法的语义生成任务迁移方法。} 与图像生成领域相比，将扩散方法应用于文本生成任务面临独特挑战：文本信号具有离散性、低信息冗余性和序列长度可变性。基于这些特点，本文从离散扩散方法出发设计了适配的训练策略和推理机制，包括集中注意力掩码机制、最佳优先推理策略和长度预测模块。相比传统的自回归方法，基于扩散方法的语义生成迁移框架具有显著优势：能够同时利用上下文信息增强生成质量、支持对已生成内容的灵活修改。实验表明，该方法在图像描述生成任务上达到了与成熟自回归方法相当的性能，同时在需要交互式修改图像描述的场景中表现更出色，为CLIP模型在语义生成任务上的迁移开辟了新途径。
\end{itemize}

\section{未来工作展望}
本文针对CLIP方法在视觉表征学习以及下游任务迁移两个方面提出了创新性的解决方案。然而，随着语言-图像对比学习领域的快速发展，仍有许多值得深入探索的方向。未来的研究工作可以从以下几个方面展开：
% 增强可训练数据 - 工作一
\paragraph{从图文对数据到网页数据的CLIP训练方法}
CLIP方法的视觉表征学习效果很大程度上依赖于预训练数据的语义信息质量和多样性。虽然第\ref{cha:iclip}章探讨了利用高质量图像分类数据的增强方法，但当前从网页获取图文对数据的过程忽略了大量有价值的信息：网页正文往往包含更丰富的图像相关描述，而页面中的其他图像也与目标图像形成语义关联。因此，如何有效利用网页数据中更完整的语义信息进行预训练\cite{S4, NEURIPS2024_2a952768}成为一个重要研究方向。
该方向的关键挑战在于如何有效建模网页中多图多文的结构化信息。现有方法主要有两种思路：一是利用文档对象模型将网页元素组织为树结构，并转化为序列形式处理\cite{DomLM,layoutlm}；二是采用图像形式保留网页的二维布局信息\cite{CLIPPO}。然而，这些方法或损失了空间结构信息，或降低了文本处理效率。因此，设计更优的网页数据建模方法将是未来研究的重点。

% 增强语言模型的性能 - 工作二
\paragraph{大语言模型赋能的CLIP训练方法}
% CLIP作为一种视觉-语言多模态联合预训练方法，对于视觉-语言交叉领域有重要意义。
尽管CLIP方法通过语言监督增强了视觉表征的语义理解能力，且第\ref{cha:fd}章进一步提升了其在细粒度视觉任务上的迁移表现，但研究表明\cite{bow,CLIPA}CLIP的语言模型不擅长处理复杂、长程和细微的语义关系。这主要源于图文对数据中的文本规模和知识密度远低于大语言模型的训练数据：后者的训练量通常是前者的数十倍到数百倍\cite{gpt4, dsv3, radford2021learning}，且来源丰富性更好。
现有工作主要通过两种方式整合大语言模型的能力：一是将CLIP的视觉表征接入大语言模型并进行联合微调\cite{blip-2, llava}，二是利用大语言模型指导CLIP的视觉表征学习\cite{LLM2CLIP}。然而，如何从预训练阶段就实现视觉和语言能力的深度融合，构建一个在两个模态上都具有卓越表现的统一模型\cite{gemini}，将会是未来重要研究方向。

% 处理更复杂的多模态任务 - 工作三
\paragraph{时空信息与语言信息结合的训练方法}
目前CLIP方法只建模了图像与文本间的对应关系，这与人类在真实世界中的学习过程有显著差异。人类视觉系统不仅需要处理三维空间信息，还需要理解时序变化，通过持续的观察、交互和思考来建立对世界的认知。因此，如何扩展当前预训练框架，使其具备对空间几何和时序信息的建模能力，同时通过语言模态理解语义信息，将对具身智能和机器人应用具有重要意义。
% 近期提出的视觉语言导航\cite{vln}和视觉-空间智能评测\cite{VSIBench}等任务为这一方向提供了良好的验证平台。这些任务要求模型同时具备空间场景理解、语言指令解析和时序推理能力。然而，如何设计合适的预训练方法来获取这些能力仍是一个开放问题。
% 正如ImageNet数据集\cite{deng2009imagenet}推动了视觉模型发展，Conceptual Captions\cite{sharma-etal-2018-conceptual,changpinyo2021conceptual}促进了视觉-语言预训练，Common Crawl\cite{cc,pile}加速了大语言模型进步
数据驱动是深度学习方法的重要思想。获取高质量的时空-语言对齐数据是该领域的关键挑战。与易于获取的图文对数据不同，包含空间信息的真实场景数据采集成本高昂且规模有限。近期生成式模型的突破\cite{latentdiff,veo2}为解决这一问题提供了新思路：通过可探索三维空间的生成办法\cite{genex, genie2}，有望大规模合成高质量的时空-语言训练数据，为下一代训练方法开辟新途径。

% CLIP作为一种视觉-语言多模态训练方法，其本质是一种图像-文本多模态训练方法。但现实世界中的视觉信号形式丰富多样，图像信息只是其中一种。若将模型训练过程类比为一个刚出生的婴儿的学习过程，那么语言-图像对比学习方法的学习过程类似于完成图像与文字的连连看游戏。但人的视觉系统是三维的，同时人的活动是有时序性的。每一个婴儿都在三维空间中不断地观察、感知、交互、思考、行动，从而建立起对世界的认知和对语言的理解。这与当前语言-图像对比学习方法的训练过程大相径庭，也说明了语言-图像对比学习方法的局限性。如何得到一个既能够处理三维空间信息和时序信息，又能理解人类丰富语言信息的视觉-语言多模态训练方法将是未来的一个重要方向，对于具身智能和机器人应用有重要意义。

% 最近提出的视觉语言导航\cite{vln}任务就是一类很好的评测方法。这类任务考验了模型在三维世界里观察环境的能力，同时需要模型对语言指示有足够的理解能力，并在时序上基于过去信息进行推理和规划。与之类似的还有视觉-空间智能评测任务\cite{VSIBench}，这类任务要求模型对一段给定视频里的空间场景进行理解和记忆，并针对以自然语言给出的问题进行推理和回答。虽然最近的研究工作在评测方法设计上有较多进展，但如何进一步扩展语言-图像对比学习方法，使其具备对几何空间和时序信息的建模能力还不清楚。

% 数据驱动性是深度学习领域的重要思想，所以数据往往被视为优先事项。ImageNet数据集\cite{deng2009imagenet}的出现推动了有监督视觉模型的繁荣。以Conceptual Captions\cite{sharma-etal-2018-conceptual,changpinyo2021conceptual}为例的数据集出现，推动了语言-图像对比学习方法的研究。Common Crawl\cite{cc,pile}等互联网级文本数据的出现推进了大语言模型的发展。如何构造时空信息与语言信息对齐的训练数据是该方向最本质的问题。与图文对数据不同，虽然互联网上有很多包含时序信息的视频数据可以作为训练数据，但空间信息很少以电子化的方式存在，而且在现实场景中采集这样的信息成本又较高，因此这种训练数据的规模较小，不足以支撑模型预训练。得益于生成式内容的技术发展\cite{latentdiff,veo2}，越来越多的研究工作开始设计可探索三维空间的生成办法\cite{genex, genie2}。这些生成式模型的出现为大规模、低成本合成时空信息与语言信息结合的训练数据提供了新的可能性，也为未来研究提供了新的方向。