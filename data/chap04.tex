% !TeX root = ../main.tex
\chapter{基于特征图自蒸馏增强的细粒度视觉任务迁移方法}
\label{cha:fd}

“预训练-微调”是计算机视觉领域基于深度学习的重要范式，深刻影响了领域内一系列重要工作\cite{HinSal06,alexnet,rcnn13,long2015fully}。第\ref{cha:iclip}章中讨论的图像分类预训练方法是最常用的预训练方法之一，亦可用于增强语言-图像对比学习方法视觉表征的语义建模能力，提高模型在零样本开放集合图像识别和图文跨模态检索任务上的表现。然而，与不依赖语义信息的像素级自监督预训练方法\cite{he2022masked}相比，CLIP方法在细粒度视觉任务上的迁移表现欠佳。虽然第\ref{cha:iclip}章中介绍的iCLIP方法显著改善了视觉-语言表征的对齐效果，但其在下游视觉任务上的迁移性能提升有限。因此，充分发挥CLIP方法的语义表征优势，增强其在下游视觉任务上的迁移性能，并实现对像素级自监督预训练方法的性能超越是本章的研究目标。

% 通过这种有监督预训练方法得到的模型权重，是各种下游视觉任务微调阶段的初始化权重。


\section{引言}
\label{sec:fd-intro}

CLIP方法通过海量互联网图文数据对进行预训练，一方面解决了图像分类方法对高质量人工标注数据的依赖，另一方面将图像标注的语义信息从固定集合的单一类别标签扩展为内容丰富、语义多样的自然语言描述。因此，CLIP方法的视觉模型展现出强大的语义建模能力，在固定视觉模型的线性探测分类任务上可以取得优异的表现。%在开放集合图像识别任务和图文跨模态检索任务上表现优异。
% 解决了图像分类低噪声有标注数据获取难和有限标签限制监督信号语义信息两大难题，因而可以扩展到利用互联网级规模的数据进行预训练，并展现出令人印象深刻的语义建模能力，从而逐步代替了以图像分类任务为代表的有监督预训练方法。
% 关于这一点，第\ref{cha:iclip}章在零样本或少样本图像识别任务和图文检索任务上进行了充分的性能分析，但实验结果也反映出了CLIP方法相比于图像分类方法在下游视觉任务上的迁移表现，并没有取得和其在图像识别或图文检索任务中一致的明显性能提升。iCLIP方法在CLIP方法基础上引入低噪声的图像分类数据，也并未观察到在下游视觉任务迁移性能上的显著改进。

与CLIP方法的思路相反，自监督预训练方法不以利用语义信号为主要目标，而是从图像数据自身寻找结构信息和监督信号。受自然语言处理领域的掩码语言模型\cite{BERT}相关工作启发，基于掩码图像模型\cite{bao2021beit, xie2022simmim, he2022masked}的自监督预训练方法要求模型通过给定的部分图像输入，预测被掩码区域的图像内容，从而在视觉模型预训练中引入像素级训练信号。这种像素级自监督方法可扩展性良好，在各类下游视觉任务中展现出优异的迁移性能，已经成为最重要的自监督预训练方法之一。本章主要讨论其中的代表性工作MAE\cite{he2022masked}。与像素级自监督方法相比，CLIP方法的视觉模型在许多下游视觉任务，尤其是依赖密集感知能力的细粒度视觉任务上并无优势。这一现象与理论预期存在明显矛盾：线性探测分类任务性能直接反映了CLIP方法视觉表征的语义表达能力，而具有更丰富语义信息的视觉表征理应在下游任务中获得更好的性能表现。
% \todo{可能需要一个MAE v.s. CLIP的图？要不然都不知道MAE是什么}

% 比较两种不同的预训练方法可以发现，语言-图像对比学习方法利用大规模图文数据对学习了丰富的语义信息，因此在固定视觉模型的线性探测分类任务上可以取得非常优异的表现。然而，相比于像素级自监督方法，语言-图像对比学习方法得到的视觉模型在许多下游视觉任务，尤其是依赖密集感知能力的细粒度视觉任务上的迁移性能并无优势。这种现象看似矛盾：线性探测性能反映了视觉表征的表达能力，而具有更丰富语义信息的表征理应在下游任务中表现更好。

% 特别地，前期实验发现，在细粒度视觉任务上相比于MIM方法表现甚至更弱。
% 因此这就提出了一个进一步的问题：CLIP方法得到的视觉模型能否在下游视觉任务，尤其是细粒度视觉任务微调方面取得与MIM方法一样的效果，甚至超越MIM方法得到的视觉模型？

针对这一问题，本章提出像素级训练信号对于提升预训练模型在下游细粒度视觉任务上的迁移性能具有关键作用。%\cite{xie2021propagate} 发展历史上实例级自监督任务与像素级自监督任务的对比，
% 为了验证这两项因素分别对实例级语言监督预训练方法CLIP的模型迁移能力的影响，需要在现有方法中引入相应设计。尽管对齐输入完整性的差异相对简单，但要将CLIP的训练目标的粒度从实例级扩展到像素级是一个重大挑战，因为这种方案不符合大规模互联网数据的基本形态，（图像，替代文本）对往往只反映实例级的对应信息，缺乏对细粒度关系的标注。
然而，CLIP作为一种基于实例级对比学习的方法，仅建模了图像全局表征与文本全局表征之间的对齐关系。由于互联网图文数据对仅提供实例级别的对齐信号，与像素级对齐所需的细粒度标注存在根本冲突，因此将CLIP方法的训练目标从实例级扩展到像素级是一个重大挑战。%，（图像，替代文本）对往往只反映实例级的对应信息，缺乏对细粒度关系的标注。
尽管已有研究（如局部化叙事\cite{LocNar}）提出了一些更高效的数据标注方法以引入像素级图文对齐的标注信息，但这类方法的数据标注成本仍然较高，难以获取大规模数据。这一局限性使其与CLIP方法依赖海量图文数据对的设计理念存在冲突。
% 违背了以CLIP为例预训练方法的重要出发点。

为了解决像素级训练信号缺失的问题，同时避免大规模像素级数据标注成本，本章研究并借鉴了一项传统方法：知识蒸馏\cite{hinton2015knowledge}。知识蒸馏是一种将训练好的模型（教师模型）的知识转移到待训练模型（学生模型）中的方法。因此知识蒸馏方法常被用于模型压缩场景，即将参数量较大的教师模型的知识转移到参数量较小的学生模型中，从而达到节约应用成本的目的。
% 从广义上说，人工标注数据驱动的深度学习方法也是一种知识蒸馏方法。这一过程中人类被视作教师模型，而神经网络被视为学生模型。
而知识蒸馏方法的核心思想，是通过教师模型将具有复杂结构信息的输入数据转化为更明确且易于学习的训练信号，用于驱动学生模型训练。

本章提出知识蒸馏方法可以作为一种训练信号的转换机制，不需要额外的数据标注即可在CLIP方法中引入像素级训练目标。同时，这种方法不用重新进行昂贵的预训练，既保留了教师模型蕴含的语义信息，又提升了学生模型在下游视觉任务上的迁移表现。具体而言，本章提出了一种称为特征图自蒸馏的知识蒸馏方法，简称为FD方法。特征图自蒸馏方法以通过CLIP方法预训练好的模型作为教师模型。在不失一般性的前提下，本章使用最常用的OpenAI CLIP\cite{radford2021learning}模型作为教师模型。给定任一图像，该方法先用权重固定的CLIP模型提取图像对应的输出特征图，再将这些特征图作为蒸馏目标，训练一个随机初始化的、与教师模型参数量相同的学生模型，最终将其迁移至下游视觉任务中。
% 整个过程如图\ref{fig:fd-overall}所示，输入图像的黄色框代表对原始图像进行随机裁剪的数据增强方法，输出特征图中的橙色块对应着描述了图像全局信息的特殊标记[\texttt{CLS}]。
% 特征图自蒸馏方法与传统的未归一化概率知识蒸馏的方法\cite{hinton2015knowledge,deit}不同。一方面前者采用特征图作为蒸馏信号，而后者采用模型输出的类别未归一化概率作为蒸馏信号，另一方面前者以引入像素级训练目标为主要目的，需要尽可能保留教师模型能力，因此使用参数量一致的学生模型，而后者则主要被用于模型压缩场景，常使用参数量小得多的学生模型。

此外，本章还提出在蒸馏过程中引入一些适当的归纳偏置和正则化，以进一步提升学生模型在下游视觉任务上的迁移性能。本章设计了以下改进措施：1）对教师模型的特征图进行标准化，放大特征图中的细微信息，并稳定训练信号的值域，便于蒸馏阶段的超参数调整；2）在教师模型和学生模型之间使用不对称比例的随机深度，减少学生模型过拟合风险，增强视觉表征的鲁棒性，同时确保教师模型产生稳定且准确的蒸馏信号；3）引入层间共享的相对位置偏置，帮助学生模型学习视觉表征的平移不变特性。

% 以CLIP模型为教师模型，并应用前述特征图自蒸馏方法得到一个新的学生模型，称为FD-CLIP模型。
基于前述的特征图自蒸馏方法，本章以CLIP模型为教师模型，训练得到了一个学生模型，并将其命名为FD-CLIP。该模型在保留了教师模型语义信息的同时，在下游视觉任务上展现出更优异的迁移性能。与原始CLIP模型相比，FD-CLIP模型在各种视觉任务上均取得明显改进：在图像分类任务中，Top-1准确率提高了2.1\%；在语义分割任务中，mIoU得分提高了2.2；在目标检测和实例分割任务中，检测框mAP和分割掩码mAP得分分别提高了3.2和2.7；在深度估计任务中，均方根误差（RMSE）降低了0.064。使用更大尺寸的CLIP模型作为教师模型时，对应的FD-CLIP模型在ImageNet-1K数据集上取得89.0\%的Top-1准确率，超过了一些使用10倍预训练数据的方法。此外，本章发现特征图自蒸馏方法还可以应用于其他非CLIP方法预训练的模型，例如实例级自监督模型DINO\cite{dino}、图像分类有监督模型DeiT\cite{deit}和有三十亿参数的大规模视觉模型SwinV2-G\cite{swinv2cvpr}。特征图自蒸馏之后，这些模型在各种下游任务迁移中取得一致的性能提升。特别地，经过特征图自蒸馏的FD-SwinV2-G模型在MSCOCO数据集\cite{chen2015microsoft}的目标检测任务上取得了64.2的检测框mAP，刷新了当时该任务的记录。

FD-CLIP模型在下游任务上的优异迁移性能表明，尽管其学习自教师模型，但像素级训练目标的引入可能赋予了学生模型与教师模型不同的、有益于下游任务迁移性能的特征属性\cite{xie2023revealing}。为了验证这一假设并深入理解特征图自蒸馏方法的工作机制，本章引入了多种模型特征属性诊断工具来分析不同方法间的模型差异。通过这些诊断工具，本章发现具有像素级训练目标的掩码图像模型展现出了一系列优异的特征属性，而经过特征图自蒸馏的FD-CLIP模型表现出类似的性质。这些分析为理解特征图自蒸馏方法如何改进下游视觉任务的迁移性能提供了更深入的见解。具体而言，特征图自蒸馏后的CLIP模型具有如下性质：1）增加了较深的层中不同注意力头的感受野多样性；2）增强了视觉表征的平移不变特性；3）使下游任务的损失景观平坦化，增加迁移过程的稳定性。

综上所述，本章的主要内容安排如下：
\begin{itemize}
    \item 第\ref{sec:fd-intro}节指出了CLIP方法在下游细粒度视觉任务上的欠佳表现，介绍了通过特征图自蒸馏引入像素级训练信号的方案，并提出利用模型特征属性诊断工具分析自蒸馏前后模型的行为差异。
    \item 第\ref{sec:fd-method}节介绍了本章的研究方法，包括特征图自蒸馏方法具体做法、蒸馏过程中的归纳偏置和正则化方法设计以及各类模型特征属性诊断工具的可视化方式与主要目的。
% 包括从对比学习角度理解图像分类任务的具体设计以及引入外部专家知识库对类别语义进行增强的具体方法等，并提出深度融合的统一预训练方法iCLIP的具体形式。
    \item 第\ref{sec:fd-result}节介绍了本章的实验设置、评测指标和模型在各类下游任务的迁移实验，以及各模块的消融分析，并给出特征图自蒸馏方法在其他模型上应用的实验结果。
    \item 第\ref{sec:fd-summary}节对本章内容进行了总结。
\end{itemize}

\section{研究方法}
\label{sec:fd-method}

\subsection{语言-图像对比学习方法与掩码图像模型对比}
\begin{figure}
  \centering
  \includegraphics[width=1.0\linewidth]{figures/fd-coco-abl.pdf}
  \caption{MAE、CLIP和FD-CLIP模型在目标检测任务上的损失曲线对比}
    % \todo{图2：在 MSCOCO 对象检测任务上微调 MAE、CLIP 和 FD-CLIP。我们可视化了 $L_{cls}$ 和 $L_{bbox}$ 相对于训练时期的损失曲线。虽然 CLIP 预训练与 $L_{cls}$ 的 MAE 预训练相当，但它显示出更差的定位能力，反映在 $L_{bbox}$ 曲线上。}
    % 还有一个lvis的，可以画在ablation里
  \label{fig:fd-coco-abl}
\end{figure}

% 语言-图像对比学习方法能整合通过对比巨大的图像文本对学到的丰富语义而闻名，与MIM方法（例如MAE）相比，CLIP方法得到的视觉模型与人类概念联系更紧密。然而，它令人印象深刻的语义能力似乎对下游任务微调的好处帮助有限。

本节首先分析了CLIP方法和MAE方法在MSCOCO\cite{chen2015microsoft}数据集目标检测任务上的表现。
目标检测任务主要包含目标定位与物体识别两个任务\cite{ren2016faster}。前者属于需要密集感知能力的细粒度任务，而后者则属于对语义能力要求较高的实例级任务，因此可以用于观察不同预训练方法的视觉模型在两种不同任务上的行为表现。
图\ref{fig:fd-coco-abl}中展示了不同预训练模型在MSCOCO数据集上的训练损失曲线。在物体识别训练损失$L_{cls}$上，MAE方法和CLIP方法之间的表现非常接近，但MAE方法预训练的模型具有更好的细粒度感知能力，因此其目标定位损失$L_{bbox}$相较于CLIP方法得到的模型更低。
这些差异促使本章进一步研究影响CLIP方法迁移性能的关键因素，以更好地释放其预训练中掌握的语义信息，同时增强其处理细粒度视觉任务的能力。

\begin{table}
\caption{
不同预训练方法的输入完整性、训练目标粒度和损失函数设计表格}
% ，除了MAE方法之外，BEiT也是一个典型的像素级自监督预训练方法。
\centering
  \begin{tabular}{lcccc}
    \toprule
  方法 & 输入完整性 & 训练目标粒度 & 损失函数设计 & 是否使用语义信息 \\
  \midrule
  BEiT & 部分图像 & 像素级任务 & 交叉熵损失函数 & \\ 
  MAE & 部分图像 & 像素级任务 & 回归损失函数 & \\
  % SimMIM~\cite{xie2021simmim}? & partial? & token-level & regression & \\ % if we mentioned them, we need to put it in table1?
  \midrule
  CLIP & 完整图像 & 实例级任务 & 交叉熵损失函数 & $\checkmark$  \\
  % 新方法 & Full & Token-level & \color{gray}{Regression} & $\checkmark$ \\
\bottomrule
  \end{tabular}
\label{tab:fd-differences}
\end{table}

% 要回答这个问题，本章首先将这些预训练方法的要素分解为三个方面：输入完整性、训练目标粒度和损失函数设计。
% 如表\ref{tab:fd-differences}所示，通过比较CLIP与两种典型MIM方法之间的设计要素差异，可以排除了损失函数设计为主要影响因素，并推测输入完整性（是否使用对完整图像还是部分图像进行建模）和训练目标粒度（是实例级监督任务还是像素级监督任务）是可能因素。
% 结合自监督预训练任务发展历史上实例级自监督任务与像素级自监督任务的对比\cite{xie2021propagate}，像素级训练目标粒度的设计可能对模型在下游任务的迁移效果以及在细粒度视觉任务上的表现起到关键因素。
% 为了验证这两项因素分别对实例级语言监督预训练方法CLIP的模型迁移能力的影响，需要在现有方法中引入相应设计。尽管对齐输入完整性的差异相对简单，但要将CLIP的训练目标的粒度从实例级扩展到像素级是一个重大挑战，因为这种方案不符合大规模互联网数据的基本形态，（图像，替代文本）对往往只反映实例级的对应信息，缺乏对细粒度关系的标注。
% 尽管有一些工作如局部化叙事\cite{LocNar}（Localized Narratives）提出了一些新的相对高效的数据标注方法来引入像素级对齐的信息，但这样的方法的数据可扩展性还是很低，违背了以CLIP为例预训练方法的重要出发点。

为进一步分析，本节将不同预训练方法的核心要素分解为三个方面：输入完整性、训练目标粒度和损失函数设计。表\ref{tab:fd-differences}中展示了CLIP方法和两个典型的掩码图像模型：MAE与BEiT\cite{bao2021beit}方法之间的设计差异。对比MAE与BEiT方法可以看到，两种掩码图像模型分别使用了回归损失函数和交叉熵损失函数，因此可以排除损失函数设计对细粒度视觉任务性能的影响。基于上述分析，本节考虑对齐掩码图像模型，使用与其一致的输入完整性和训练目标粒度设计来增强CLIP方法在下游视觉任务中的迁移性能。
% 并推测输入完整性（是否使用对完整图像还是部分图像进行建模）和训练目标粒度（是实例级监督任务还是像素级监督任务）是可能因素。
% ：输入完整性、训练目标粒度和损失函数设计，本节考虑对齐MIM方法设计中使用的输入完整性和训练目标粒度来增强CLIP模型的迁移性能。
在这两个方面中，虽然使用部分图像作为CLIP方法的输入相对容易\cite{FLIP}，但因为大规模获取图像像素与文本内容细粒度对齐的图文数据对非常困难，所以将CLIP方法的训练目标粒度从实例级扩展到像素级仍是一个重大挑战。此外，CLIP方法的预训练成本较高。以OpenAI CLIP模型为例，其预训练过程中使用超过4亿张训练图片和数百张显卡训练数周，才能达到较好效果。因此，本节方法旨在避免重新预训练且不依赖额外数据标注的前提下，实现在CLIP方法中引入部分图像的输入设计和像素级的训练目标粒度设计。

\subsection{特征图自蒸馏方法}

\begin{figure}
  \centering
  \includegraphics[width=0.8\linewidth]{figures/fd-overall.pdf}
  \caption{利用特征图自蒸馏方法引入像素级训练信号的流程图}
  % \todo{图 1：特征图蒸馏图，引入令牌级目标来蒸馏预训练的 CLIP 模型。橙色块代表 [\texttt{CLS}] 令牌，橙色框表示原始图像的随机裁剪。}
  \label{fig:fd-overall}
\end{figure}

% 整个过程如图\ref{fig:fd-overall}所示，输入图像的黄色框代表对原始图像进行随机裁剪的数据增强方法，输出特征图中的橙色块对应着描述了图像全局信息的特殊标记[\texttt{CLS}]。

% 特征图自蒸馏方法与传统的未归一化概率知识蒸馏的方法\cite{hinton2015knowledge,deit}不同。一方面前者采用特征图作为蒸馏信号，而后者采用模型输出的类别未归一化概率作为蒸馏信号，另一方面前者以引入像素级训练目标为主要目的，需要尽可能保留教师模型能力，因此使用参数量一致的学生模型，而后者则主要被用于模型压缩场景，常使用参数量小得多的学生模型。

受知识蒸馏方法启发，本节提出利用知识蒸馏作为训练目标转换方法，将CLIP方法的训练目标粒度从实例级扩展到像素级，同时最大程度保留预训练CLIP模型的语义信息。
整个特征图自蒸馏过程如图\ref{fig:fd-overall}所示。输入图像的黄色框代表对原始图像进行随机裁剪的数据增强方法，输出特征图中的橙色块对应着描述了图像全局信息的特殊标记[\texttt{CLS}]。
和传统知识蒸馏方法类似，特征图自蒸馏方法使用预训练好的CLIP模型充当教师模型并固定参数不动，同时随机初始化一个新模型作为学生模型，来尽可能保留教师模型预训练过程中建模的语义信息。%，同时在此阶段引入像素级任务以增强视觉模型在细粒度任务上的迁移表现。

和传统知识蒸馏方法不同，特征图自蒸馏方法有如下两个特征：
\begin{itemize}
    \item 采用预训练模型的完整输出特征图作为蒸馏目标，而不是以未归一化概率为蒸馏目标\cite{hinton2015knowledge,deit}。该做法一方面可以引入像素级训练目标，并为学生模型提供更多可蒸馏信号以尽可能保留教师模型的能力。另一方面，因为很多预训练模型\cite{dino}并没有未归一化概率的输出形式，该做法也使得特征图自蒸馏方法可以运用到更多预训练模型中。
    此外，为了确保特征图逐像素对齐，特征图自蒸馏方法需要对输入教师模型和学生模型的图像应用相同的图像增强方法。
    后续实验表明，与蒸馏其他实例级特征相比，蒸馏整个特征图可以更显著地增强CLIP方法在下游视觉任务的迁移效果，验证了之前关于像素级训练目标对模型细粒度视觉任务迁移效果重要性的猜想。
    \item 使用相同大小的教师模型和学生模型，而不是大参数的教师模型和小参数的学生模型。这种做法的原因是特征图自蒸馏方法并非为了压缩模型，而是为了在引入像素级训练目标的同时，尽可能使得学生模型保留教师模型中蕴含的语义信息和视觉能力。
    虽然学生模型的训练目标是尽可能模仿教师模型的输出特征图，但从随机初始化开始训练允许学生模型有不同的优化路径。这种优化路径上的差异为学生模型提供了具备与掩码图像模型相似特征属性的可能性。
\end{itemize}

% \begin{figure}
%   \centering
%   \includegraphics[width=1.0\linewidth]{figures/fd-framework-update.pdf}
%   \caption{新引入的“特征图自蒸馏”阶段在整个“预训练-微调”范式中的作用}
%     % \todo{图2：在 MSCOCO 对象检测任务上微调 MAE、CLIP 和 FD-CLIP。我们可视化了 $L_{cls}$ 和 $L_{bbox}$ 相对于训练时期的损失曲线。虽然 CLIP 预训练与 $L_{cls}$ 的 MAE 预训练相当，但它显示出更差的定位能力，反映在 $L_{bbox}$ 曲线上。}
%   \label{fig:fd-framework-update}
% \end{figure}

\begin{table}
\caption{特征图自蒸馏方法的输入完整性、训练目标粒度和损失函数设计表格}
\centering
  \begin{tabular}{lcccc}
\toprule
  方法 & 输入完整性 & 训练目标粒度 & 损失函数设计 & 是否带语义信息 \\
  \midrule
  MAE & 部分图像 & 像素级任务 & 回归损失函数 & \\
  % SimMIM~\cite{xie2021simmim}? & partial? & token-level & regression & \\ % if we mentioned them, we need to put it in table1?
  \midrule
  CLIP & 完整图像 & 实例级任务 & 交叉熵损失函数 & $\checkmark$  \\
  FD-CLIP & 完整图像 & 像素级任务 & 回归损失函数 & $\checkmark$ \\
\bottomrule
  \end{tabular}
\label{tab:fd-method}
\end{table}

% 因此，该方法称作“特征图自蒸馏”方法，因其使用的蒸馏目标是特征图，教师模型是它自身。
如表\ref{tab:fd-method}所示，通过以上设计，特征图自蒸馏方法得以在保留预训练CLIP模型语义信息的同时，不需要额外数据标注即可引入像素级训练信号，从而达到以较小的代价增强CLIP方法在下游视觉任务迁移性能的目的。% 新引入的“特征图自蒸馏”方法在整个“预训练-微调”范式中xxx。该阶段在不重新预训练CLIP模型的前提下，以较小的成本引入像素级监督任务，可以理解为是针对预训练模型的一次“中间训练”（Mid-training）。

\subsection{归纳偏置和正则化方法设计}
基于特征图自蒸馏方法的双分支结构，本节提出在蒸馏过程中引入额外的归纳偏置和正则化方法，以进一步提升学生模型在下游视觉任务上的迁移性能。
% 接着，在“特征图自蒸馏”方法基础上，本节设计并引入以下归纳偏置和正则化策略，以进一步提高学生模型的迁移效果。
\paragraph{对教师模型的输出特征图进行标准化} 不同预训练模型的输出特征图有各自的数值范围，直接回归这些特征图使得蒸馏过程中需要针对不同模型调整超参数，限制了方法的通用性。此外，一些特征图中的细节信息被编码在相对小的数值中。如果不对这些细微信号进行放大，学生模型在蒸馏过程中往往会忽略这些信息。为了解决这些问题，本节提出使用标准化操作对教师网络的输出特征图进行后处理，使得不同输出特征图的均值为0，方差为1。% https://ssjcoding.github.io/2019/03/27/normalization-and-standardization/
该操作可通过无参数的层归一化\cite{ba2016layer}实现。接着，特征图自蒸馏方法在学生和教师模型特征图之间采用平滑$\ell_{1}$损失函数进行监督，以减少离异特征值的影响。% https://blog.csdn.net/ai_faker/article/details/117414614
以$s$和$t$分别表示学生和教师模型的输出特征图，特征图自蒸馏方法公式化如下：
\begin{equation}
    \mathcal{L} (s, t) = \begin{cases}
        \frac{1}{2} (g(s) - t')^2/\beta, & | g(s) - t' | \leq \beta \\
        |g(s)-t'|-\frac{1}{2}\beta, & | g(s) - t' | > \beta,
    \end{cases}
    \label{eq:iclip-distill}
\end{equation}
其中$\beta$值设为2，而$t'$表示对教师输出特征图$t$进行标准化后的特征图。$g$是单个线性层，实现轻量级的投影变换以允许教师和学生模型之间有不同的输出特征图维度，从而增强了该方法的通用性。在CLIP方法训练过程中，视觉模型通过特殊标记[\texttt{CLS}]聚合图像全局信息并与语言表征进行对齐，因此[\texttt{CLS}]标记相比于特征图中的其他位置蕴含了更丰富的语义信息。考虑到这一点，在特征图自蒸馏方法中，[\texttt{CLS}]标记对应的蒸馏损失权重为其他位置的10倍，从而增强学生模型保留教师模型语义信息的效果。

\paragraph{不对称比例的随机深度} 特征图自蒸馏方法的双分支结构允许在教师模型和学生模型上应用不对称的正则化强度。实验发现，这种策略可以使学生模型在下游视觉任务的迁移性能更佳。具体来说，特征图自蒸馏过程中会在学生模型分支上添加一定比例的随机深度正则化\cite{huang2016deep}，而在教师模型分支上不添加任何正则化。这种设计既保证了教师模型蒸馏信号的稳定性，不对学生模型的训练造成干扰，又减轻了学生模型的过拟合风险。

\paragraph{层间共享的相对位置偏置} 预训练的CLIP模型采用绝对位置编码策略，也即通过对不同位置的输入标记使用不同的索引来进行区分。最近的工作\cite{Swin,li2022mvitv2}发现相对位置偏置增强了视觉表征的平移不变性，因此对模型的迁移性能有一定好处。
由于特征图自蒸馏方法仅依赖于模型最后一层特征图，因此该方法可以灵活调整学生模型的位置编码策略。实验表明层间共享的相对位置偏置策略\cite{bao2021beit}能够提升模型的迁移性能。这种策略要求模型所有层使用相同的相对位置偏置矩阵，从而缓解深层网络中因为不同位置间特征相似性过高导致的表征坍缩的问题\cite{xie2023revealing}。% 实验表明，层间共享的相对位置偏置的总体性能最佳。
通过模型特征属性诊断工具可以发现使用这种位置编码策略使得模型不同注意力头的感受野更加多样化，允许模型建模更多不同位置的图像信息。这一现象在较深的模型层中更为明显。% （\todo{如补充材料中的图所示}）

\subsection{模型特征属性的诊断工具}
为进一步理解特征图自蒸馏前后模型行为的变化，本节引入多个特征属性诊断工具对不同的模型进行对比分析，以提供对特征图自蒸馏方法背后机制的直观理解：
\begin{itemize}
    \item 注意力头级别的诊断工具：平均注意力距离\cite{dosovitskiy2020vit,xie2023revealing}。本章使用的视觉模型结构为Vision Transformer\cite{dosovitskiy2020vit}（ViT）。该模型结构中的自注意力层建模了不同像素位置间的相关关系，而每个自注意力层包含多个自注意力头用于建模不同的像素间关系。平均注意力距离的诊断工具统计了每个像素与其他像素间经过注意力权重加权后的相对距离的平均值，因此反映了每个自注意力头的感受野范围大小。在统计过程中，由于特殊标记[\texttt{CLS}]不包含任何位置信息，因此不会纳入统计范围。
    \item 模型层级别的诊断工具：平均注意力图\cite{zhou2021deepvit}。通过平均所有注意力头的注意力图可以得到层级别的平均注意力图。注意力图中有两种常见的模式：对角线模式和竖线模式。对角线模式表明该模型更多地依赖于来自相对位置关系的视觉线索，具有更好的视觉表征平移不变性，这是对各种下游视觉任务迁移性能的有益属性。同时，越是靠近中心对角线的对角线模式越能反映出模型学到了一些基于图像连续性的局部先验。而竖线模式则表明处于某些固定位置的图像输入对所有其他位置的特征产生了较大影响，这种模式反映了模型提取的视觉表征往往是平移变化的，抗干扰能力较弱。
    \item 模型级别的诊断工具：归一化的损失景观\cite{li2018visualizing}。这一诊断工具通过对迁移后的模型权重施加一系列不同程度的高斯噪声进行扰动，并观察扰动后的模型性能变化。噪声强度根据每个权重的$\ell_2$范数来决定，以消除不同模型权重幅度不同的影响。通常而言，损失局部极小值附近损失景观越平坦，则模型的优化过程更稳定、泛化性能更佳\cite{li2018visualizing}。
\end{itemize}

\section{实验结果}
\label{sec:fd-result}

% 本节展示了在已经训好的CLIP模型基础之上引入特征图自蒸馏阶段对下游任务迁移性能的提升情况，并对该方法的一些关键设计、推广性和诊断性质进行了深入分析。
第\ref{sec:fd-exp-setting}节介绍了特征图自蒸馏阶段和后续视觉任务迁移的实验设置。第\ref{sec:fd-exp-clip}节中展示了特征图自蒸馏后的CLIP模型在各类下游视觉任务的性能提升情况，并在第\ref{sec:fd-exp-input}节讨论了输入图像完整性和不同训练目标粒度的影响。第\ref{sec:fd-exp-detail}节包含归纳偏置和正则化方法对于模型迁移性能影响的消融实验，并在第\ref{sec:fd-more-models}节展示了将特征图自蒸馏方法应用到其他预训练模型后对下游任务迁移性能的提升情况。
最后，在第\ref{sec:fd-analysis}节中运用模型特征属性的诊断工具对蒸馏前后的模型进行了分析，以深入理解该方法特性。

\subsection{实验设置}
\label{sec:fd-exp-setting}
\paragraph{蒸馏阶段实验设置} 所有实验均使用ImageNet-1K\cite{deng2009imagenet}训练图像进行特征图自蒸馏。特征图自蒸馏方法以教师模型的输出特征图为监督目标，不依赖图像类别等人工标注信息，因此也可以在别的图像数据集上进行应用。除非特别说明，所有实验都在ImageNet-1K数据集（IN-1K）上蒸馏了100轮，并默认使用ViT-Base\cite{dosovitskiy2020vit}结构的CLIP模型（CLIP-Base）进行实验。%\todo{其他详细信息在补充材料中。}
\paragraph{迁移阶段实验设置} 为充分评测模型在各类下游视觉任务上的迁移效果，本节选择了4个下游任务：ImageNet-1K数据集\cite{deng2009imagenet}上的图像分类任务、ADE20K数据集\cite{zhou2019ade}上的语义分割任务、MSCOCO数据集\cite{chen2015microsoft}上的目标检测和实例分割任务以及NYUv2\cite{NYUv2}数据集上的深度估计任务。除图像分类任务外，其余三项任务均为细粒度视觉任务。各任务的具体设置如下：
\begin{itemize}
    \item \textbf{图像分类任务}~~ 该任务的迁移方法按照BEiT方法\cite{bao2021beit}的设置，使用AdamW优化器\cite{adamw}、逐层衰减的学习率和$224 \times 224$的输入图像分辨率。CLIP-Base模型在该任务上迁移100轮，而使用ViT-Large结构的CLIP模型（CLIP-Large）则迁移50轮。线性探测分类任务迁移实验则按照MAE方法的设置，使用LARS优化器\cite{lars}和0.1的学习率训练90轮，并将权重衰减设置为0。该任务使用ImageNet-1K评测集上的Top-1准确率进行报告。
    \item \textbf{语义分割任务}~~  该任务按照Swin方法\cite{Swin}中的描述，使用常用的UPerNet框架\cite{xiao2018upernet}进行迁移。迁移时使用AdamW优化器，以32的批大小训练8万个数据批次。其他超参数设置：学习率为4e-4、学习率逐层衰减强度为0.65、权重衰减为0.05、随机深度比例为0.2。在训练中，输入图像分辨率设置为$512 \times 512$。该任务使用单尺度输入进行测试并报告模型在验证集上的mIoU得分。
    \item \textbf{目标检测和实例分割任务}~~  该任务迁移阶段采用常见方法\cite{cae}的设置，包括使用MaskRCNN框架\cite{Mask-rcnn}在MSCOCO数据集上训练12轮，并使用多尺度训练和单尺度测试策略。为了降低全局自注意力层在高分辨率图像上的训练代价和内存开销，在该任务迁移时采用了滑动窗口注意力机制\cite{Swin}，并将窗口的像素大小设置为$224 \times 224$，使其与预训练和蒸馏阶段中使用的图像尺寸一致。为了聚合图像的全局信息，在模型最后添加了一个额外的全局自注意力层。其他超参数设置如下：迁移过程中的批大小为16、最大学习率为2e-4、学习率逐层衰减系数为0.75，并遵循惯例，在第9和第11轮将学习率降低至先前的0.1倍。该任务使用模型在开发集上的检测框mAP和分割掩码mAP得分进行报告。
    \item \textbf{深度估计任务}~~  该任务的迁移阶段采用已有工作\cite{glpdepth, xie2023revealing}的设置，使用其中的24000张图像作为训练集迁移25轮，并在包含654张图像的官方测试集上进行评测，涵盖了215个不同室内场景的深度估计任务。其他超参数设置如下：输入图像被随机裁剪为$480 \times 480$的分辨率、迁移过程中的批大小为24、学习率为5e-5。该任务在测试时平均了两个方形窗口的预测结果，并使用模型在评测集上的均方根误差（RMSE）进行报告。
\end{itemize}

\paragraph{蒸馏阶段和迁移阶段的超参数设置}
表\ref{tab:fd-hyper-pretrain}中详细描述了蒸馏阶段和图像分类任务迁移阶段的超参数设置。

\begin{table}
    \centering
    \caption{特征图自蒸馏方法在蒸馏阶段和迁移阶段的超参数设置}
    \begin{tabular}{lcccc}
    \toprule
        & \multicolumn{2}{c}{蒸馏阶段} & \multicolumn{2}{c}{迁移阶段} \\
        \bf 超参数名称 & \bf CLIP-Base & \bf CLIP-Large & \bf CLIP-Base & \bf CLIP-Large \\
        \midrule
            图像块大小 & $16 \times 16$ & $14 \times 14$ & $16 \times 16$ & $14 \times 14$ \\
            层数  & 12 & 24 & 12 & 24\\
            隐变量维度 & 768 & 1024 & 768 & 1024\\
            前馈层中间维度 & 3072 & 4096 & 3072 & 4096 \\
            注意力头数目 & 12 & 16 & 12 & 16 \\
            注意力头宽度 & 64 & 64 & 64 & 64 \\
        \midrule
            训练轮数 & 300 & 300 & 100 & 50 \\
            批大小 & 2048 & 2048 & 2048 & 2048  \\
            输入图像分辨率 & $224 \times 224$& $224 \times 224$& $224 \times 224$& $224 \times 224$\\
            优化器参数$(\epsilon)$ & 1e-8 & 1e-8 & 1e-8 & 1e-8  \\
            优化器参数$(\beta)$ & (0.9,0.999)& (0.9,0.999)& (0.9,0.999)& (0.9,0.999)  \\
            最大学习率 & 1.2e-3 & 1.2e-3 & \{5e-3,6e-3\} & 1e-3  \\
            最小学习率 & 2e-5 & 2e-5 & 2e-6 & 2e-6 \\
            学习率调度形式 & 余弦调度 & 余弦调度 & 余弦调度 & 余弦调度 \\
            学习率热启动轮 & 10 & 10 & 20 & 5 \\
            学习率衰减系数 & 1.0 & 1.0 & \{0.6,0.65\} & 0.75 \\
        \midrule
            梯度截断系数 & 3.0 & 3.0 & 5.0 & 5.0 \\
            % Dropout & \multicolumn{4}{c}{\xmark} \\
            权重衰减系数 & 0.05 & 0.05 & 0.05 & 0.05\\
            随机深度比例 & 0.1 & 0.3 & \{0.1,0.2,0.3\} & 0.4 \\
            标签平滑比例 & - & - & 0.1 & 0.1 \\
            % 数据增强方法 & \multicolumn{2}{c}{随机缩放裁剪} & \multicolumn{2}{c}{ }\\
            % 这个列起来好长啊。。。
        \bottomrule
    \end{tabular}
    \label{tab:fd-hyper-pretrain}
\end{table}

\subsection{特征图自蒸馏后的CLIP模型迁移效果}
\label{sec:fd-exp-clip}
\paragraph{特征图自蒸馏后的CLIP-Base模型迁移效果} 本节首先在CLIP-Base模型上进行特征图自蒸馏实验，并将训练轮数扩展到300轮以充分验证结果。表\ref{tab:fd-clip_FD}展示了特征图自蒸馏后的模型在各类下游视觉任务上的迁移表现。
经过特征图自蒸馏得到的FD-CLIP模型在各类下游视觉任务上都有一定的性能提升，并在一些细粒度视觉任务上提升更为显著。相比于原始CLIP模型，FD-CLIP模型在ImageNet-1K图像分类任务上提高2.1\%的Top-1准确率，在语义分割任务上提高了2.2的mIoU得分，并在目标检测和实例分割任务上分别将检测框mAP和分割掩码mAP得分提高了3.2和2.7。此外，FD-CLIP模型在深度估计任务上也取得了明显的提升，均方根误差从0.416降低到了0.352。与此同时，FD-CLIP模型在各类下游视觉任务上的迁移表现已经超过了掩码图像模型MAE方法，验证了特征图自蒸馏能够显著提升CLIP方法在下游视觉任务上的迁移性能。

\begin{table}
\caption{特征图自蒸馏方法提升CLIP-Base模型的视觉任务迁移性能结果
% 该模型是在 ImageNet-1K 数据集 [10] 上提炼的，其中的图像只有 300 个 epoch。在四个评估基准上观察到明显的进步。MAE [17] 结果也以灰色列出以供参考。
}
\centering
% \addtolength{\tabcolsep}{-3.0pt}
% \renewcommand{\arraystretch}{1.1}
  \begin{tabular}{lccccc}
\toprule
   & IN-1K & ADE20K & \multicolumn{2}{c}{MSCOCO} & NYUv2 \\
   % \cline{2-3} \cline{5-6}
   \midrule
   % {AP$_\text{box}$} & {AP$_\text{mask}$}
方法    &   Top-1准确率 &  mIoU  & 检测框mAP & 分割掩码mAP & RMSE\scriptsize{ ($\downarrow$)}\\
  \midrule
  MAE & 83.6 & 48.1 & 46.5 & 40.9 & 0.383 \\

  \midrule
  
  CLIP & 82.9  & 49.5 & 45.0 & 39.8 & 0.416 \\
  FD-CLIP & 85.0  & 51.7 & 48.2 & 42.5 & 0.352 \\
  $\Delta$ & \textbf{$\uparrow$2.1}  &  \textbf{$\uparrow$2.2} & \textbf{$\uparrow$3.2} & \textbf{$\uparrow$2.7} & \textbf{$\downarrow$0.064}\\
\bottomrule
  \end{tabular}
\label{tab:fd-clip_FD}
\end{table}

% 在其他Validation set上的评测，表示鲁棒性
% To further evaluate the robustness of our model, we include more adversarial validation sets. Our FD-CLIP reaches 75.0\% and 43.8\% top-1 accuracy on ImageNet-v2 and ImageNet-sketch, higher than the original CLIP by \textbf{2.5\%} and \textbf{10.6\%}, respectively. These results show our method is not overfitted on ImageNet-1k validation set.

\begin{table}
\caption{特征图自蒸馏后的模型在零样本和线性探测分类任务上的表现
% \yx{Put init weight to appendix.} [这个啥意思来着？]
}
% \vspace{-0.3em}
\centering
% \addtolength{\tabcolsep}{-1.5pt}
% \renewcommand{\arraystretch}{1.1}
  \begin{tabular}{lccc}
\toprule
  评测方法 & CLIP & FD-CLIP & $\Delta$ \\
  \midrule
  零样本图像分类任务 (Top-1准确率) & 68.6 & 68.0 & -0.6 \\
  线性探测图像分类任务 (Top-1准确率) & 79.5 & 80.1 & +0.6 \\ 
\bottomrule
  \end{tabular}
\label{tab:fd-zero_shot}
\end{table}

\paragraph{特征图自蒸馏后的模型对预训练语义信息的保留情况} 
尽管特征图自蒸馏方法的训练目标与原始CLIP方法的训练目标不同，但由于蒸馏过程中学生模型充分学习了教师模型特征图，因此学生模型保留了大部分语义信息。
表\ref{tab:fd-zero_shot}比较了蒸馏前后的教师模型与学生模型在ImageNet-1K评测集上零样本和线性探测图像分类任务的Top-1准确率。结果表明蒸馏后的学生模型保留了原始CLIP模型蕴含的大部分语义信息。
% 也就是说，基于完整特征图的特征图自蒸馏方法可以在很大程度上保留了CLIP模型中包含的大量语义信息，同时可以取得优于MIM方法的迁移性能。

\paragraph{特征图自蒸馏方法的额外训练代价} 前述特征图自蒸馏方法使用ImageNet-1k数据集训练了300轮，而ImageNet-1K数据集包含128万张图像，因此总训练图像数目约为3.84亿张。OpenAI CLIP模型在预训练阶段使用私有的WIT-400M图文数据集\cite{radford2021learning}训练了32轮，因此总训练图像数目约为128亿张。由于语言模型参数量远小于视觉模型，此处忽略其预训练成本。
因此，特征图自蒸馏方法无需重新预训练或额外数据标注，通过3\%的额外训练成本，即可在各类下游视觉任务上带来大幅度性能提升。
% 这个代价是可以接受的，而且该方法不需要对CLIP进行重新训练，也不需要引入额外的数据标注代价。
此外，CLIP预训练过程依赖大量显卡才能达到足够的批大小：OpenAI CLIP模型需要256张以上显卡同时训练，而特征图自蒸馏方法对硬件要求较低，应用更为灵活。%对大多数实验室和团队都更友好。
% 原始的CLIP模型使用32768 的批大小和256个GPU进行训练，与CLIP预训练不同，特征图自蒸馏方法仅需要更小的批大小（2048），且只需要8个GPU即可训练，这对大多数实验室和团队都更友好。


\begin{table}
\caption{
特征图自蒸馏方法提升CLIP-Large模型的视觉任务迁移性能结果
% 扩展特征图自蒸馏方法至CLIP-Large模型后在图像任务上的迁移性能
}
\centering
% \addtolength{\tabcolsep}{-1.0pt}
% \renewcommand{\arraystretch}{1.1}
  \begin{tabular}{lccc}
\toprule
 方法 & 图像分辨率 & 训练数据集 & IN-1K Top-1准确率 \\
  \midrule
    WiSE-FT~\cite{ftclip2021} & 336$^2$ & WIT-400M~\cite{radford2021learning} & 87.1 \\
    DeiT III~\cite{touvron2022deit} & 384$^2$ & IN-22K~\cite{deng2009imagenet} & 87.7 \\
    ViT~\cite{dosovitskiy2020vit} & 512$^2$ & JFT-300M~\cite{Sun_2017_JFT300m} & 87.8 \\
   Scaling~\cite{zhai2022scaling} &  384$^2$ &  JFT-3B\cite{zhai2022scaling} &  88.5 \\
   BEiT~\cite{bao2021beit} &  512$^2$ &  DALLE~\cite{ramesh2021dalle1} \& IN-22K &  88.6 \\

  \midrule
  CLIP & $224^2$ & WIT-400M & 86.1 \\
  \midrule
  \multirow{3}{*}{FD-CLIP} & $224^2$ & WIT-400M & \textbf{87.7}\scriptsize{ (+1.6)} \\
   & 224$^2$ & WIT-400M \& IN-22K & \textbf{88.3}\scriptsize{ (+2.2)} \\
   & 336$^2$ & WIT-400M \& IN-22K & \textbf{89.0}\scriptsize{ (+2.9)} \\
\bottomrule
  \end{tabular}
\label{tab:fd-clip_large_FD}
\end{table}

\paragraph{特征图自蒸馏后的CLIP-Large模型迁移效果} 本组实验将特征图自蒸馏方法应用于CLIP-Large模型，以验证该方法的可扩展性。
如表\ref{tab:fd-clip_large_FD}所示，经过特征图自蒸馏的FD-CLIP模型在ImageNet-1K数据集图像分类任务中取得了87.7\%的Top-1准确率，相比原始CLIP模型提升1.6\%。
使用ImageNet-22K数据集迁移，并将图像分辨率扩展至$336\times 336$后，FD-CLIP模型在ImageNet-1K图像分类任务上取得了89.0\%的Top-1准确率，超过了使用接近10倍预训练数据方法\cite{zhai2022scaling}的性能表现。% 因为CLIP-Large模型使用14的块大小，并非2的整次幂，多分辨率的FPN与模型的块大小不兼容，因此没有在其他下游任务上进行微调比较。

\subsection{训练目标粒度和输入完整性对迁移性能的影响}
\label{sec:fd-exp-input}

\paragraph{训练目标粒度对迁移性能的影响} 本组实验研究了不同训练目标粒度对下游任务迁移性能的影响。本组实验比较了三种不同的训练目标粒度设置：
\begin{itemize}
    \item \textbf{蒸馏特殊标记特征}~~ 视觉模型中的[\texttt{CLS}]标记是一个特殊标记，并在CLIP方法中起到独特的作用：它不仅聚合了全局的图像信息，而且直接与语言表征进行对齐，因此蕴含了丰富的语义信息。在此设置中，蒸馏过程以教师模型的[\texttt{CLS}]标记输出特征作为训练目标构建了实例级训练目标粒度。
    \item \textbf{蒸馏全局平均特征}~~ 此设置是介于实例级训练目标粒度和像素级训练目标粒度的中间形式。具体来说，此设置对教师模型的输出特征图应用全局平均操作，以构造一个简化后的蒸馏目标。因此这个蒸馏目标同时包含了来自每个像素的特征信息，但又缺乏像素级的训练目标分辨率。
    \item \textbf{蒸馏完整特征图}~~ 这是特征图自蒸馏方法的默认设置，使用了教师模型的完整输出特征图作为蒸馏目标来构建像素级的训练目标粒度，并对[\texttt{CLS}]标记的对应特征使用更大的损失权重。
\end{itemize}

\begin{table}
\caption{自蒸馏过程中使用不同训练目标粒度的消融实验}
% 。这些模型是在 ImageNet-1K 数据集 [10] 上蒸馏的，有 100 个 epoch。代币级目标对于提高 CLIP 的迁移性能至关重要。
\centering
% \addtolength{\tabcolsep}{-2.5pt}
% \renewcommand{\arraystretch}{1.1}
  \begin{tabular}{lccccc}
\toprule
   & IN-1K & ADE20K & \multicolumn{2}{c}{MSCOCO} & NYUv2 \\
   % \cline{4-5}
   \midrule

   方法 &  Top-1准确率   &  mIoU  & 检测框mAP & 分割掩码mAP & RMSE\scriptsize{ ($\downarrow$)}\\
  \midrule

  MAE & 83.6 & 48.1 & 46.5 & 40.9 & 0.383 \\
    原始模型 & 82.9 & 49.5 & 45.0 & 39.8 & 0.416 \\ 
  % CLIP~\cite{radford2021learning} & 82.9 & 49.5 & 45.0 & 39.8 & 0.416 \\
  \midrule
  % FIXME: 这个要修-编译bug
  特殊标记特征 & 81.9 & 47.5 & 44.8 & 39.6 & 0.396 \\
  全局平均特征  & 83.3 & 50.3 & 46.3 & 40.6 & 0.393 \\
  完整特征图 & \textbf{84.4} & \textbf{51.8} & \textbf{47.9} & \textbf{42.2} & \textbf{0.350} \\
\bottomrule
  \end{tabular}
\label{tab:fd-ablation_targets}
\end{table}

表\ref{tab:fd-ablation_targets}展示了通过不同训练目标粒度蒸馏后的模型在下游任务上的迁移性能表现。蒸馏[\texttt{CLS}]标记特征的模型在深度估计任务上的误差有所减少，但在其他视觉任务上的性能相比原始CLIP模型没有提升，甚至有一定损失。这是因为[\texttt{CLS}]标记无法提供教师模型蕴含的全部信息。相比之下，蒸馏全局平均特征的模型在所有下游视觉任务中均展现出了一定的性能增益，但提升幅度微弱。蒸馏完整特征图的特征图自蒸馏方法引入了像素级训练目标粒度，相比于其他蒸馏目标而言，这一方法得到的模型在下游视觉任务中的迁移表现最佳，超过了MAE方法的迁移效果。结果表明像素级训练目标对提升CLIP方法的下游任务迁移性能至关重要。

\paragraph{输入完整性对迁移性能的影响} 这组实验研究了特征图自蒸馏方法中的输入完整性对下游视觉任务迁移性能的影响。为构造不同的输入图像比例，本组实验参考掩码图像模型，以块形状随机掩码部分图像后输入模型。%，比例分别设置为25\%、50\%、75\%和100\%（即完整图像）。
表\ref{tab:fd-ablation_masking}展示了对学生模型应用不同输入比例进行自蒸馏后在各类下游任务上的迁移性能，其中标注$^{\dag}$的实验表明教师模型的输入图像也缩减到对应比例，否则教师模型默认使用完整图像作为输入。

在相同的训练轮数下，使用完整图像和部分图像进行特征图自蒸馏得到的模型性能相近，但使用25\%输入比例设置的模型表现不佳。同时，教师模型的输入需要是完整图像，否则自蒸馏后的模型在一些深度估计等任务上的迁移表现欠佳。结果表明输入完整性不是影响CLIP方法在下游视觉任务上迁移效果的关键因素。
\begin{table}
\caption{自蒸馏过程中使用不同输入比例的消融实验
}
\centering
% \addtolength{\tabcolsep}{-1.5pt}
% \renewcommand{\arraystretch}{1.1}
  \begin{tabular}{lccccc}
\toprule
   & IN-1K & ADE20K & \multicolumn{2}{c}{MSCOCO} & NYUv2 \\
   % \cline{4-5}

   \midrule

   方法  &  Top-1准确率   &  mIoU  & 检测框mAP & 分割掩码mAP & RMSE\scriptsize{ ($\downarrow$)}\\
  \midrule

    MAE & 83.6 & 48.1 & 46.5 & 40.9 & 0.383 \\
    原始模型 & 82.9 & 49.5 & 45.0 & 39.8 & 0.416 \\ 
  % CLIP~\cite{radford2021learning} & 82.9 & 49.5 & 45.0 & 39.8 & 0.416 \\
\midrule
  $25\%$ 输入图像$^{\dag}$ & 83.3 & 47.8 & 45.2 & 40.1 & 0.397 \\
  $25\%$ 输入图像 & 83.1 & 48.8 & 45.1& 39.8& 0.379 \\
  $50\%$ 输入图像 & 84.2 & 51.5 & 47.5& 41.7& 0.351 \\
  $75\%$ 输入图像 & \textbf{84.4} & \textbf{52.0} & 47.8& 41.9 & \textbf{0.347} \\
  完整输入图像 & \textbf{84.4} & 51.8 & \textbf{47.9} & \textbf{42.2} & 0.350 \\
\bottomrule
  \end{tabular}
\label{tab:fd-ablation_masking}
\end{table}

\begin{table}
        \centering
        \caption{自蒸馏过程中使用部分比例输入图像并增加训练轮数的消融实验}
    \
        \begin{tabular}{lccccc}
            \toprule
               & 显卡 & IN-1K & ADE20K & MSCOCO & NYUv2 \\
               %\cline{5-6}
                \midrule
               方法 & 小时 &  Top-1准确率   &  mIoU  & 检测框mAP & RMSE\scriptsize{ ($\downarrow$)}\\
              \midrule
            %   100数据轮次 & 96.4 & 83.1 & 48.8 & 45.1& 39.8& 0.379 \\
            %   200数据轮次 & 192.8 & 83.9 & 49.7 & 46.4& 40.9& 0.366 \\
            %   400数据轮次 & 385.6 & \textbf{84.4} & 51.1 & 47.2& 41.5& 0.367 \\
            % %   $50\%$ + 100ep input & 84.2 & 51.5 & 47.5& 41.7& 0.351 \\
            % %   $50\%$ + 200ep input & 84.6 & 51.5 & 48.3& 42.3& 0.349 \\
            %   100数据轮次$^{\dag}$ & 170.7 & \textbf{84.4} & \textbf{51.8} & \textbf{47.9} & \textbf{42.2} & \textbf{0.350} \\
              100轮 & 96.4 & 83.1 & 48.8 & 45.1&  0.379 \\
              200轮 & 192.8 & 83.9 & 49.7 & 46.4& 0.366 \\
              400轮 & 385.6 & \textbf{84.4} & 51.1 & 47.2&  0.367 \\
            %   $50\%$ + 100ep input & 84.2 & 51.5 & 47.5& 41.7& 0.351 \\
            %   $50\%$ + 200ep input & 84.6 & 51.5 & 48.3& 42.3& 0.349 \\
              100轮$^{\dag}$ & 170.7 & \textbf{84.4} & \textbf{51.8} & \textbf{47.9} & \textbf{0.350} \\
            \bottomrule
        \end{tabular}
        \label{tab:fd-ablate_longer_maskinput}
\end{table}

考虑到使用25\%输入比例时，特征图自蒸馏方法的计算量相比于蒸馏完整输入图像时要更小，因此表\ref{tab:fd-ablate_longer_maskinput}研究了增加蒸馏训练轮数的影响，其中标注$^{\dag}$的实验代表蒸馏完整输入图像的实验基线。
实验结果表明，延长使用25\%输入比例的蒸馏数据至4倍训练轮数后，模型在细粒度视觉任务上迁移表现仍落后于蒸馏完整输入图像的默认设置。考虑到教师模型没法利用部分图像输入进行加速，此时实验设置需要的计算代价已经超过默认设置。因此使用完整输入图像进行蒸馏可以取得更优的迁移结果。
此外，表\ref{tab:fd-ablation_masking}的实验结果表明，使用75\%的输入比例进行特征图自蒸馏可以获得相近的迁移性能。此时对图像进行掩码可以被认为是一种数据增强方法，在减少过拟合风险的同时提供一定比例的加速效果。
% 综合以上实验可以得出结论：训练目标粒度对于提升下游视觉任务的迁移性能非常重要，输入完整性的影响不大。


\subsection{归纳偏置和正则化方法的消融}
\label{sec:fd-exp-detail}
本节讨论了特征图自蒸馏方法中不同归纳偏置和正则化方法设计对下游任务迁移性能的影响。本节实验对比了不同模型在ImageNet-1K图像分类任务上的迁移效果。

\begin{table}
    % \small
    \centering
    % \renewcommand{\arraystretch}{1.15}
    % \addtolength{\tabcolsep}{-3.5pt}
    \caption{
    特征图自蒸馏方法中归纳偏置和正则化方法的消融实验}
    \begin{tabular}{lccc}
        \toprule
        （a）特征图后处理设计 & 无 & $\ell_2$ 归一化 & 标准化 \\
        \midrule
        IN-1K (Top-1 准确率) & 83.5 & 83.9 & \textbf{84.4} \\
        \midrule
        （b）学生/教师的随机深度比例 & 0.1 / 0.1 & 0.1 / 0 & 0.2 / 0  \\
        \midrule
        IN-1K (Top-1 准确率) & 84.0 & \textbf{84.4} & 84.0 \\
        \midrule
        （c）位置编码设计 & 绝对位置编码 & 不共享相对编码 & 共享相对编码 \\
        \midrule
        IN-1K (Top-1 准确率) & 84.0 & 83.9 & \textbf{84.4} \\
        \bottomrule
    \end{tabular}
    \label{tab:fd-ablation_tricks}
\end{table}

\paragraph{不同教师特征图后处理方式的设计} 表\ref{tab:fd-ablation_tricks} （a）中对比了不同教师特征图后处理方式的效果。
与直接使用原始教师模型特征图作为蒸馏信号相比，教师模型特征图标准化带来了0.9\%的Top-1准确率提升。进一步对比$\ell_{2}$归一化方法和标准化方法，后者仍有0.5\%的增益。这种后处理方法使得特征图自蒸馏阶段的超参数对使用何种预训练模型不敏感，因此本章所有实验均采用标准化方式对教师模型的输出特征图进行后处理。

\paragraph{不对称比例随机深度的设计} 表\ref{tab:fd-ablation_tricks} （b）分析了不同程度的随机深度正则化的影响。实验表明在学生模型上适当使用一定比例的随机深度正则化对迁移性能有一定帮助，原因在于正则化减轻了蒸馏过程中过拟合的风险。但是，在教师模型上使用随机深度正则化对模型迁移性能有一定损害，这说明稳定的教师模型输出更有益。因此本章所有实验默认采用这种不对称比例随机深度的策略。

\paragraph{不同位置编码配置的设计} 表\ref{tab:fd-ablation_tricks} （c）探讨了学生模型使用不同位置编码配置后在下游任务上的迁移表现。结果表明，层间共享的相对位置编码配置优于其他位置编码配置。根据特征属性分析，这种位置编码配置可以使得模型具有更多样的图像感受野，增强了模型的表达能力。

\subsection{特征图自蒸馏方法增强其他预训练模型的迁移效果}
\label{sec:fd-more-models}


\begin{table}
\caption{
特征图自蒸馏方法提升DINO和DeiT模型的视觉任务迁移性能结果
% 这些模型是在 ImageNet-1K 数据集 [10] 上蒸馏的，并训练300个数据轮次。
}
\centering
% \addtolength{\tabcolsep}{-1.5pt}
% \renewcommand{\arraystretch}{1.1}
  \begin{tabular}{lccccc}
\toprule
   & IN-1K & ADE20K & \multicolumn{2}{c}{MSCOCO} & NYUv2 \\
   \midrule
方法    &  Top-1准确率   &  mIoU  & 检测框mAP & 分割掩码mAP & RMSE\scriptsize{ ($\downarrow$)}\\
  \midrule
  DINO & 82.8 & 46.2 & 45.8 & 40.7 & 0.412 \\
  FD-DINO & 83.8 & 47.7 & 46.1 & 40.9 & 0.394 \\
  $\Delta$ & \textbf{$\uparrow$1.0} & \textbf{$\uparrow$1.5} & \textbf{$\uparrow$0.3} & \textbf{$\uparrow$0.2} & \textbf{$\downarrow$0.018} \\
  \midrule
  
  DeiT & 81.8 & 47.0 & 45.8 & 40.7 & 0.403 \\
  FD-DeiT & 83.0 & 48.0 & 46.4 & 41.0 & 0.404 \\
  $\Delta$ & \textbf{$\uparrow$1.2} & \textbf{$\uparrow$1.0} & \textbf{$\uparrow$0.6} & \textbf{$\uparrow$0.3} & \textbf{$\uparrow$0.001} \\
\bottomrule
  \end{tabular}
\label{tab:fd-extend_dino_deit}
% \vspace{-1.0em}
\end{table}

\begin{table}
        \centering
    \caption{
特征图自蒸馏方法提升MAE模型的视觉任务迁移性能结果
}
        \begin{tabular}{lccccc}
            \toprule
               & IN-1K & ADE20K & \multicolumn{2}{c}{MSCOCO} & NYUv2 \\
               \midrule
              方法 &  Top-1准确率   &  mIoU  & 检测框mAP & 分割掩码mAP & RMSE\scriptsize{ ($\downarrow$)}\\
              \midrule
              MAE & 83.6 & 48.1 & 46.5 & 40.9 & 0.383 \\
              FD-MAE & 83.4 & 47.9 & 46.7 & 41.2 & 0.364 \\
              $\Delta$ & \textbf{$\downarrow$0.2} & \textbf{$\downarrow$0.2} & \textbf{$\uparrow$0.2} & \textbf{$\uparrow$0.3} & \textbf{$\downarrow$0.019} \\
            \bottomrule
        \end{tabular}
        \label{tab:fd-extend_mae}
\end{table}

表\ref{tab:fd-clip_FD}和表\ref{tab:fd-clip_large_FD}中展示的实验结果证明了在CLIP模型上应用特征图自蒸馏方法，可以有效改进其在下游视觉任务上的迁移表现。虽然本章工作主要针对CLIP模型，但本节实验验证了特征图自蒸馏方法对其他预训练模型的作用，从而进一步考察了该方法的通用性。
表\ref{tab:fd-extend_dino_deit}展示了在实例级自监督方法的DINO模型\cite{dino}和图像分类有监督方法的DeiT模型\cite{deit}上应用特征图自蒸馏方法后，模型在各类下游视觉任务迁移性能上的改进。
DINO模型和DeiT模型在预训练过程中均未引入带有像素级训练目标的视觉任务，因此其性质上与CLIP方法有一定的相似性。
实验结果表明，特征图自蒸馏方法对这些预训练方法的模型也有一定帮助，尤其提升了它们在语义分割和目标检测任务上的表现，但对它们在深度估计任务上的好处并不普遍。

虽然MAE方法在预训练阶段已经包括了像素级的视觉训练目标，但仍然可以对其应用特征图自蒸馏方法，并观察特征图自蒸馏方法对下游视觉任务性能的影响。表\ref{tab:fd-extend_mae}展示了以MAE模型为教师模型进行特征图自蒸馏方法后得到的模型在下游任务上的迁移结果。
实验结果表明FD-MAE模型在大多数视觉任务上的迁移表现与蒸馏前的教师模型的表现类似，仅在深度估计任务上有一定改进。%，但不如在CLIP方法上应用特征图自蒸馏后的改进明显。
这一结果验证了本章假设，即特征图自蒸馏方法的主要收益来自于低成本地引入像素级训练目标，因此掩码图像模型无法继续从特征图自蒸馏方法中获益。

\begin{table}
\caption{
特征图自蒸馏方法提升SwinV2-G模型的视觉任务迁移性能结果}
\centering
% \addtolength{\tabcolsep}{-3.5pt}
% \renewcommand{\arraystretch}{1.1}
  \begin{tabular}{lcccc}
  
\toprule

   &  IN-1K & \multicolumn{2}{c}{MSCOCO}  & ADE20K  \\
  \midrule
  方法 &Top-1 准确率 & 检测框mAP & 分割掩码mAP & mIoU \\
  \midrule
  GLIPv2-CoSwin-H~\cite{GLIPv2_2022} & - & 62.4 & - & - \\
  Florence-CoSwin-H~\cite{yuan2021florence} & - & 62.4 & - & -  \\
  DINO-Swin-L~\cite{zhang2022dino} & - & 63.3 & - & - \\
  MaskDINO-Swin-L~\cite{li2022mask} & - & - & 54.7 & 60.8 \\
  ViT-Adapter-L~\cite{chen2022vitadapter} & - & - & - & 60.5 \\
    
\midrule
  SwinV2-G & 89.2 & 63.1 & 54.4 & 59.9 \\
  FD-SwinV2-G & \textbf{89.4}\scriptsize{ (+0.2)} & \textbf{64.2}\scriptsize{ (+1.1)} & \textbf{55.4}\scriptsize{ (+1.0)} & \textbf{61.4}\scriptsize{ (+1.5)} \\
%      
  
\bottomrule
  \end{tabular}
\label{tab:fd-swinv2_G}
\end{table}

此外，为了进一步验证特征图自蒸馏方法的模型可扩展性，本组实验选用超大规模的SwinV2-G\cite{swinv2cvpr}模型进行实验。相比于前述使用的最大模型CLIP-Large，SwinV2-G模型含有超过三十亿个模型参数，模型规模扩大了10倍。
如表\ref{tab:fd-swinv2_G}所示，经过特征图自蒸馏的FD-SwinV2-G模型在ADE20K数据集的语义分割任务和MSCOCO数据集的目标检测任务上实现了61.4 mIoU得分和64.2 检测框mAP得分，相比于原先的最好方法MaskDINO-Swin-L和DINO-Swin-L\cite{zhang2022dino,li2022mask}分别提升了0.6和0.9。
FD-SwinV2-G模型与原始SwinV2-G模型使用相同的语义分割和目标检测迁移框架\cite{xiao2018upernet, chen2019htc}，并保持评测设置不变，但相比于原始SwinV2-G模型，FD-SwinV2-G模型在目标检测和实例分割任务上继续取得超过1.0的mAP得分提升，并在语义分割任务上提升1.5的mIOU得分。
结果表明，特征图自蒸馏方法具有良好的通用性，可适用于不同预训练方法、网络结构和模型规模，提升它们在下游视觉任务上的迁移性能。


\subsection{模型特征属性分析}
\label{sec:fd-analysis}

本节使用第\ref{sec:fd-method}节介绍的模型特征属性诊断工具对蒸馏前后的CLIP模型进行分析，同时与掩码图像模型MAE方法进行对比，以进一步揭示特征图自蒸馏方法如何影响了模型特征属性。本节分析均使用ImageNet-1K评测集的图像。
\begin{figure}
  \centering
  \includegraphics[width=1.0\linewidth]{figures/fd-attn-distance.pdf}
  \caption{不同预训练模型的平均注意力距离分布}
  \label{fig:fd-attn-distance}
\end{figure}
\paragraph{感受野多样化的注意力头} 该诊断工具分析了模型不同注意力头的图像感受野多样性。图\ref{fig:fd-attn-distance}分别展示了MAE模型、原始CLIP模型和经过特征图自蒸馏的FD-CLIP模型中不同网络深度的模型层内每个注意力头的平均注意力距离。
可以看到，在所有模型中，深度较浅的模型层里不同注意力头的感受野范围比较发散，说明此时模型既能建模局部的图像信息，又能获取全局的图像信息，展现了注意力头间的多样性。
然而，这种感受野的多样性在原始CLIP模型的较深模型层中迅速缩减。这表明模型的表达能力没有得到充分利用\cite{xie2023revealing}。
相比之下，FD-CLIP模型缓解了这个问题。FD-CLIP模型不同注意力头的感受野多样性在较深的模型层中保留得更好，也与MAE模型特征更为相似。%，表明它的模型容量可能被利用地更好。

\begin{figure}
  \centering
  \includegraphics[width=1.0\linewidth]{figures/fd-attn-map.pdf}
  \caption{不同预训练模型的平均注意力图表现}
  \label{fig:fd-attn-map}
\end{figure}

\paragraph{平移不变性增强的注意力图} 图\ref{fig:fd-attn-map}展示了不同模型中某一层的平均注意力图。与原始CLIP模型相比，MAE模型的平均注意力图显示出更多的对角线模式，表明其更注重来自相对位置的视觉信息。这种特征属性表明掩码图像模型可以驱使模型学习到更强的视觉表征平移不变性。这种局部敏感性使得模型在需要密集感知能力的细粒度下游任务上有更好的迁移表现。
而CLIP模型在更深的层（7至11层）中展现出了更多垂直模式，这表明CLIP模型的视觉表征被一些处于绝对位置的输入像素主导。特征图自蒸馏之后，FD-CLIP模型的平均注意力图中垂直模式部分消失，视觉表征的平移不变性得到增强，与MAE模型特征更为相似，因此有助于提高其下游任务迁移性能。

\begin{figure}
  \centering
  \includegraphics[width=1.0\linewidth]{figures/fd-loss-landscape.pdf}
  \caption{不同预训练模型的损失景观和准确率景观}
  \label{fig:fd-loss-landscape}
\end{figure}

\paragraph{损失景观的平坦化} 图\ref{fig:fd-loss-landscape}显示了不同模型在不同噪声强度影响下的损失景观和准确率景观\cite{li2018visualizing}。从可视化结果中可以看出，MAE模型和FD-CLIP模型的损失景观相比于原始CLIP模型的损失景观更加平坦，表明了经过特征图自蒸馏的模型在下游视觉任务的优化过程更加平稳，泛化性能更好。% 这一观察结果也与它在实验中更好的微调效果一致。

% 图 3：（a） MAE [17]、（b） CLIP [42] 和 （c） FD-CLIP 上每个头部在每一层深度的平均注意力距离。距离是在像素级别测量的。

% 图 4：（a） MAE [17]、（b） CLIP [42] 和 （c） FD-CLIP 上的平均注意力图。这些地图是所有头部和所有图像的平均值。选择五个代表性图层，即第 0、1、7、9、11 个图层，以节省空间。完整的注意力地图可以在补充材料中找到。

% 图5：（a）平均绝对误差（MAE）[17]、（b）对比语言-图像预训练（CLIP）[42] 以及（c）FD - CLIP的损失/准确率态势图[33]，其中x轴表示噪声强度，y轴表示损失/准确率。每张图展示了使用5个随机生成方向得到的5个态势图。 

\begin{figure}
  \centering
  \includegraphics[width=1.0\linewidth]{figures/fd_compare_all_ape_rpe.pdf}
  \caption{不同位置编码策略的平均注意力距离分布}
  \label{fig:fd_compare_all_ape_rpe}
\end{figure}

\paragraph{位置编码策略对注意力头多样性的影响} 特征图自蒸馏方法在学生模型中引入了层间共享的相对位置编码设计，以减少在较深的模型层中出现不同像素表征间过于相似的现象。表\ref{tab:fd-ablation_tricks}（c）中的消融实验结果表明这种归纳偏置对CLIP模型的迁移性能有改进作用。
本节分析了在特征图自蒸馏过程使用不同位置编码配置的模型特征属性区别，包括绝对位置编码、不共享相对位置编码和共享相对位置编码。图\ref{fig:fd_compare_all_ape_rpe}中展示了在使用不同位置编码配置的情况下模型各注意力头的平均注意力距离分布。
与使用绝对位置编码和不共享相对位置编码的模型相比，使用共享相对位置编码的模型在更深的模型层中注意力头的感受野大小更加多样化，表明此时模型深层参数的冗余性更低。这一观察解释了这种共享相对位置编码具有更佳迁移效果的原因。


\section{总结}
\label{sec:fd-summary}


% CLIP 模型已经展示了令人印象深刻的高零镜头识别精度，但是，它们在下游视觉任务上的迁移性能并不理想。相反，尽管在训练过程中没有语义标签，但掩码图像模型 （MIM） 在对下游任务的微调方面表现异常出色。我们注意到这两个任务具有不同的成分：图像级目标与标记级目标，交叉熵损失与回归损失，以及全图像输入与部分图像输入。为了减少差异，我们引入了一个经典的特征图蒸馏框架，它可以同时保留 CLIP 模型的语义能力，同时构建一个包含 MIM 关键成分的任务。实验表明，特征图蒸馏方法显著提高了 CLIP 模型在几个典型的下游视觉任务上的迁移性能。我们还观察到该方法产生了新的 CLIP 表示，这些表示与 MIM 的表示具有一些共享的诊断特性。此外，特征图蒸馏方法推广到其他预训练模型，如 DINO、DeiT 和 SwinV2-G，在 MSCOCO 对象检测方面达到了 64.2 mAP 的新纪录，提高了 +1.1。

% 我们的贡献总结如下：
% ・我们研究了 CLIP 和 MIM 方法之间的成分差异，并证明靶标粒度对于 MIM 在微调中的成功至关重要。
% ・我们利用经典的特征图蒸馏将 CLIP 的训练目标粒度转换为令牌级的粒度，从而增强了其 performance 并保留其语义信息。
% ・我们在特征图自蒸馏过程中提出了几种进一步扩大改进的关键技术，包括蒸馏标准化特征图、不对称滴路径速率和共享相对位置偏置。
% ・通过多种诊断工具，我们发现与 CLIP 相比，MIM 和 FD-CLIP 都具有几个直观上良好的特性，这可能为它们卓越的迁移性能提供见解。
% ・我们将我们的方法推广到各种预训练模型，并观察到一致的收益。我们还通过使用我们的框架改进先进的 3B SwinV2-G 模型，创造了 MSCOCO 对象检测的新记录。

% 背景+motivation
语言-图像对比学习方法利用大规模图文数据对学习了丰富的语义信息，在零样本开放集合图像识别任务上表现优异。然而，CLIP方法在许多下游视觉任务，尤其是依赖密集感知能力的细粒度视觉任务上的迁移表现不佳。
% 前一章中提出的iCLIP方法通过引入已有的低噪声有标注数据，有效地扩展了CLIP模型训练可利用数据源的同时，显著提高了数据利用的效率。但这种方法得到的CLIP模型在迁移到下游视觉任务，尤其是细粒度视觉任务上的性能提升有限。
% 语言-图像对比学习方法利用大规模图文数据对学习了丰富的语义信息，因此在固定视觉模型只微调分类器的线性探测分类任务上可以取得非常优异的表现。然而，相比于像素级自监督方法，语言-图像对比学习方法得到的视觉模型在许多下游视觉任务，尤其是依赖密集感知能力的细粒度视觉任务上的迁移性能并无优势。

% 解决方案和发现
本章提出了特征图自蒸馏方法，将掩码图像模型的像素训练目标引入CLIP方法中，在无需大规模人工标注图文数据对和重新预训练CLIP模型的情况下，即可显著提升CLIP方法的细粒度视觉任务迁移性能。
该方法以CLIP模型自身作为教师模型，提取其输出特征图作为随机初始化的学生模型的训练目标。特征图自蒸馏方法一方面无需数据标注即可构造像素级训练目标，另一方面以较低代价将教师模型的知识转移到学生模型中。经过特征图自蒸馏的CLIP模型在语义分割、目标检测、深度估计等细粒度下游视觉任务上取得明显性能提升。将该方法推广到参数量达三十亿的SwinV2-G模型后，经过特征图自蒸馏的SwinV2-G模型取得了当时MSCOCO数据集上目标检测任务的新纪录。此外，本章还提出多种模型特征属性诊断工具对自蒸馏前后的模型进行分析，揭示了自蒸馏后的CLIP模型与掩码图像模型具有的相似性质，从而解释了特征图自蒸馏方法的有效性原因。
% 实验观察到像素级图像自监督预训练模型，如掩码图像模型（MIM），在下游任务上的迁移表现优异。受该类方法启发，本章仔细研究了两种不同预训练任务的建模区别，并着重讨论了输入图像完整性和训练目标粒度对视觉任务迁移性能的影响。
% 为低代价地在CLIP模型中引入像素级训练目标，同时避免大规模的数据标注以利用互联网级数据，本章提出使用特征图自蒸馏的方法，在尽可能保留CLIP模型中蕴含的语义信息同时引入像素级的监督目标。这种方法无须重新训练CLIP模型，且相比于原本CLIP模型训练代价而言，只需要额外3\%的训练成本即可显著提升其在各类下游任务，尤其是细粒度视觉任务上的性能表现。
% 通过在特征图自蒸馏框架中设计不同训练目标粒度和不同输入图像比例的实验，结果表明提升下游任务迁移性能的关键在于应用像素级的训练目标粒度。
% 此外，本章还提出利用多种注意力头级、模型层级、模型级的诊断工具对自蒸馏前后的模型进行分析和理解，可视化观察显示经过特征图自蒸馏的FD-CLIP在直观上拥有一些和MIM模型类似的特性，这为它们优异的迁移性能提供了见解。
% 为进一步验证该框架的通用性和可扩展性，本章将该框架用于不同视觉模型结构、不同预训练任务和不同模型规模的视觉模型，并观察到一致的收益。通过特征图自蒸馏框架改进的有三十亿参数的SwinV2-G模型，创造了当时MSCOCO数据集上目标检测的新纪录。

% 引出下一章
虽然本章工作提升了CLIP方法在下游视觉任务上的迁移性能，但其在语义生成任务上的迁移方式仍待探索。下一章以图像描述生成任务为主要场景，研究了CLIP方法在语义生成任务上的迁移方式，并提出了基于离散扩散模型的迁移方法。